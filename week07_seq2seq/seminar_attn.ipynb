{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "* Alexandr Panin, Arseniy Ashuha, you can text me ```ars.ashuha@gmail.com```,\n",
    "* Based on https://github.com/ebenolson/pydata2015\n",
    "\n",
    "\n",
    "<h1 align=\"center\"> Part I: Attention mechanism at toy problems </h1> \n",
    "\n",
    "<img src=\"https://s2.postimg.org/pq18f5t7t/deepbb.png\" width=480>\n",
    "\n",
    "In this seminar you will implement attention mechanism and apply it to a simple task of associative recall.\n",
    "\n",
    "# Install me:\n",
    "```(bash)\n",
    "sudo pip install --upgrade https://github.com/yandexdataschool/agentnet/archive/master.zip\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import numpy as np\n",
    "from lasagne.layers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import theano,theano.tensor as T\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem description:\n",
    "\n",
    "You are given a sequence of pairs [key,value]. \n",
    "\n",
    "Both keys and values are one-hot encoded integers. \n",
    "\n",
    "The network should learn to generate values in order of ascension of keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CODE_SIZE = 10\n",
    "def generate_sample(min_length = 3, max_length = 10, code_size=CODE_SIZE):\n",
    "    assert code_size >= max_length\n",
    "    length = np.random.randint(min_length, max_length)\n",
    "    \n",
    "    keys = np.random.permutation(length)\n",
    "    values = np.random.permutation(length)\n",
    "    input_pairs = zip(keys,values)\n",
    "    \n",
    "    input_1hot = np.zeros([length+1,code_size*2])\n",
    "    for i,(k,v) in enumerate(input_pairs):\n",
    "        input_1hot[i+1][k] = 1\n",
    "        input_1hot[i+1][code_size + v] = 1\n",
    "    \n",
    "    sorted_pairs = sorted(input_pairs,key=lambda (k,v):k)\n",
    "    \n",
    "    target_1hot = np.zeros([length+1,code_size*2])\n",
    "    for i,(k,v) in enumerate(sorted_pairs):\n",
    "        target_1hot[i+1][k] = 1\n",
    "        target_1hot[i+1][code_size + v] = 1\n",
    "    \n",
    "    \n",
    "    return input_1hot,target_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inp,out = generate_sample(max_length=5,code_size=5)\n",
    "print '-'*9 + \"KEY\" + '-'*9 + ' ' + '+'*9 + \"VAL\" + \"+\"*9\n",
    "print \"Input pairs:\\n\",inp\n",
    "print \"Target pairs:\\n\",out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention!\n",
    "\n",
    "We're now going to implement attention mechanism, or more specifically, _additive attention_ (a.k.a. Bahdanau's attention).\n",
    "\n",
    "We'll do so in two steps:\n",
    "\n",
    "* __AttentionWeights(encoder_seq,attn_query)__ - a layer that returns attention weights (aka probabilities of taking each value).\n",
    "* __AttentionOutput(encoder_seq,attn_weights)__ - a layer that averages inputs given probabilities from AttentionWeights.\n",
    "\n",
    "If you're not feeling familiar with this procedure, just follow the step-by-step instructions in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.init import Normal\n",
    "class AttentionWeights(MergeLayer):\n",
    "    def __init__(self, encoder_seq, attn_query, num_units):\n",
    "        MergeLayer.__init__(self, [encoder_seq, attn_query])\n",
    "        \n",
    "        enc_units = encoder_seq.output_shape[2]\n",
    "        dec_units = attn_query.output_shape[1]\n",
    "        \n",
    "        self.W_enc = self.add_param(Normal(), (enc_units, num_units), name='enc_to_hid')\n",
    "        self.W_query = self.add_param(Normal(), (dec_units, num_units), name='dec_to_hid')\n",
    "        self.W_out = self.add_param(Normal(), (num_units, 1),name='hid_to_logit')\n",
    "    \n",
    "    def get_output_for(self, inputs):\n",
    "        # the encoder_sequence shape = [batch, time,units]\n",
    "        # the query shapeshape  = [batch, units]\n",
    "        encoder_sequence, query = inputs\n",
    "        \n",
    "        # Hidden layer activations, shape [batch,seq_len,hid_units]\n",
    "        \n",
    "        query_to_hid = query.dot(self.W_query)[:,None,:]\n",
    "        \n",
    "        enc_to_hid = <Your code: contributon from encoder to hid, shape:[batch,time,units]>\n",
    "        \n",
    "        hid = T.tanh(<add up hiddens>)\n",
    "        \n",
    "        # Logits from hidden, [batch_size, seq_len]\n",
    "        logits = <Your code: get logits, shape: [batch,time]>\n",
    "        \n",
    "        assert logits.ndim ==2, \"Logits must have shape [batch,time] and be 2-dimensional.\"\\\n",
    "                                \"Current amount of dimensions:\"+str(logits.ndim)\n",
    "        \n",
    "        attn_weights = T.nnet.softmax(logits)\n",
    "        \n",
    "        return attn_weights\n",
    "    \n",
    "    def get_output_shape_for(self,input_shapes):\n",
    "        enc_shape,query_shape = input_shapes\n",
    "        return enc_shape[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AttentionOutput(MergeLayer):\n",
    "    def __init__(self, encoder_seq, attn_weights):\n",
    "        MergeLayer.__init__(self,[encoder_seq,attn_weights])\n",
    "    \n",
    "    def get_output_for(self,inputs):\n",
    "        # encoder_sequence shape = [batch,time,units]\n",
    "        # attn_weights shape = [batch,time]\n",
    "        encoder_sequence, attn_weights = inputs\n",
    "    \n",
    "        #Reshape attn_weights to make 'em 3-dimensional: [batch,time,1] - so you could multiply by encoder sequence\n",
    "        attn_weights = attn_weights.reshape([attn_weights.shape[0],attn_weights.shape[1],1])\n",
    "        \n",
    "        #Compute attention response by summing encoder elements with weights along time axis (axis=1)\n",
    "        attn_output = <Compute attention response by summing encoder elements with weights along time axis (1)>\n",
    "        \n",
    "        return attn_output\n",
    "    \n",
    "    def get_output_shape_for(self,input_shapes):\n",
    "        enc_shape,query_shape = input_shapes\n",
    "        return (enc_shape[0],enc_shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a single step of recurrent neural network using attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = T.itensor3(\"Input tokens [batch,time,code]\")\n",
    "reference_answers = T.itensor3(\"Reference answers[batch,time,code]\")\n",
    "\n",
    "l_inputs = InputLayer((None,None,CODE_SIZE*2),input_sequence)\n",
    "l_prev_answers = InputLayer((None,None,CODE_SIZE*2),reference_answers[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.memory import RNNCell\n",
    "class step:\n",
    "    prev_output = InputLayer((None, CODE_SIZE*2), name='previous output')\n",
    "    input_sequence = InputLayer((None, None, CODE_SIZE*2), name='input sequence for attention')\n",
    "    prev_rnn = InputLayer((None, 64), name='last rnn state')\n",
    "    \n",
    "    #TODO your code here\n",
    "    attention_weights = AttentionWeights(input_sequence, prev_rnn,32)\n",
    "    attention_value = AttentionOutput(input_sequence, attention_weights)\n",
    "    \n",
    "    new_rnn = RNNCell(prev_rnn,concat([attention_value, prev_output]))\n",
    "    \n",
    "    output_probs = DenseLayer(\n",
    "        concat([new_rnn,attention_value]),\n",
    "        num_units=CODE_SIZE*2, nonlinearity=T.nnet.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet import Recurrence\n",
    "#This layer applies RNN to itself in a symbolic loop.\n",
    "#Please wait for DeepBayes' staff to explain how it works.\n",
    "\n",
    "rnn = Recurrence(\n",
    "    input_sequences    = {step.prev_output: l_prev_answers},\n",
    "    input_nonsequences = {step.input_sequence: l_inputs},\n",
    "    state_variables    = {step.new_rnn: step.prev_rnn},\n",
    "    tracked_outputs    = [step.output_probs,step.attention_weights],\n",
    "    unroll_scan=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_probs,attn_weights = get_output(\n",
    "    [rnn[step.output_probs], rnn[step.attention_weights]])\n",
    "\n",
    "predict = theano.function(\n",
    "    [input_sequence,reference_answers],\n",
    "    [output_probs,attn_weights],\n",
    "    allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next_answers = reference_answers[:,1:]\n",
    "\n",
    "loss = -T.log(output_probs)*next_answers -T.log(1-output_probs)*(1-next_answers)\n",
    "loss = T.mean(loss)\n",
    "\n",
    "updates = <define updates with any method you like>\n",
    "\n",
    "train = theano.function([input_sequence, reference_answers], loss, updates=updates,allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tnrange\n",
    "from IPython.display import clear_output\n",
    "loss_history = []\n",
    "\n",
    "for i in tnrange(10000):\n",
    "    bx,by = generate_sample()\n",
    "    loss_history.append(train([bx],[by]))\n",
    "    \n",
    "    if i%500==0:\n",
    "        clear_output(True)\n",
    "        plt.plot(loss_history)\n",
    "        plt.show()\n",
    "        \n",
    "        #draw attention map\n",
    "        bx,by = generate_sample()\n",
    "        probs,attentions = predict([bx],[by])\n",
    "\n",
    "        input_kv = zip(bx[:,:CODE_SIZE].argmax(-1),bx[:,CODE_SIZE:].argmax(-1))\n",
    "        target_kv = zip(by[:,:CODE_SIZE].argmax(-1),by[:,CODE_SIZE:].argmax(-1))\n",
    "        plt.imshow(attentions[0])\n",
    "        plt.xticks(*zip(*enumerate(map(str,input_kv))),rotation=45)\n",
    "        plt.yticks(*zip(*enumerate(map(str,target_kv))),rotation=45)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "widgets": {
   "state": {
    "1efdd72be63d457dafc441ce841e39f5": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
