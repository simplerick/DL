{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Deep Learning (7 points)\n",
    "\n",
    "Today we're gonna apply the newly learned DL tools for sequence processing to the task of predicting job salary.\n",
    "\n",
    "Special thanks to [Oleg Vasilev](https://github.com/Omrigan/) for the assignment core (orignally written for theano/tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the challenge\n",
    "For starters, let's download the data from __[here](https://yadi.sk/d/vVEOWPFY3NruT7)__.\n",
    "\n",
    "You can also get it from the competition [page](https://www.kaggle.com/c/job-salary-prediction/data) (in that case, pick `Train_rev1.*`).\n",
    "\n",
    "\n",
    "Our task is to predict one number, __SalaryNormalized__, in the sense of minimizing __Mean Absolute Error__.\n",
    "\n",
    "<img src=\"https://kaggle2.blob.core.windows.net/competitions/kaggle/3342/media/salary%20prediction%20engine%20v2.png\" width=400px>\n",
    "\n",
    "To do so, our model ca access a number of features:\n",
    "* Free text: __`Title`__ and  __`FullDescription`__\n",
    "* Categorical: __`Category`__, __`Company`__, __`LocationNormalized`__, __`ContractType`__, and __`ContractTime`__.\n",
    "\n",
    "\n",
    "You can read more [in the official description](https://www.kaggle.com/c/job-salary-prediction#description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "      <th>Log1pSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>193962</th>\n",
       "      <td>71774524</td>\n",
       "      <td>C NET Developer With Silverlight  Manchester C...</td>\n",
       "      <td>C .NET Developer With Silverlight  Manchester ...</td>\n",
       "      <td>Lancashire Manchester M21 0</td>\n",
       "      <td>Manchester Science Park</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>MNM Associates LTD</td>\n",
       "      <td>IT Jobs</td>\n",
       "      <td>30000.00 - 35000.00 GBP Annual</td>\n",
       "      <td>32500</td>\n",
       "      <td>jobserve.com</td>\n",
       "      <td>10.389026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166313</th>\n",
       "      <td>71296911</td>\n",
       "      <td>Excel Administrator</td>\n",
       "      <td>An established corporate company in Northampto...</td>\n",
       "      <td>Northamptonshire</td>\n",
       "      <td>Northamptonshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>contract</td>\n",
       "      <td>Office Angels</td>\n",
       "      <td>Admin Jobs</td>\n",
       "      <td>8/hour</td>\n",
       "      <td>15360</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "      <td>9.639587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139918</th>\n",
       "      <td>70566202</td>\n",
       "      <td>Registered Nurses</td>\n",
       "      <td>The Medic Alert Foundation provides lifesaving...</td>\n",
       "      <td>Milton Keynes</td>\n",
       "      <td>Milton Keynes</td>\n",
       "      <td>part_time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medical Alert</td>\n",
       "      <td>Healthcare &amp; Nursing Jobs</td>\n",
       "      <td>14,285</td>\n",
       "      <td>14285</td>\n",
       "      <td>jobstoday.co.uk</td>\n",
       "      <td>9.567036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id                                              Title  \\\n",
       "193962  71774524  C NET Developer With Silverlight  Manchester C...   \n",
       "166313  71296911                                Excel Administrator   \n",
       "139918  70566202                                  Registered Nurses   \n",
       "\n",
       "                                          FullDescription  \\\n",
       "193962  C .NET Developer With Silverlight  Manchester ...   \n",
       "166313  An established corporate company in Northampto...   \n",
       "139918  The Medic Alert Foundation provides lifesaving...   \n",
       "\n",
       "                        LocationRaw       LocationNormalized ContractType  \\\n",
       "193962  Lancashire Manchester M21 0  Manchester Science Park          NaN   \n",
       "166313             Northamptonshire         Northamptonshire          NaN   \n",
       "139918                Milton Keynes            Milton Keynes    part_time   \n",
       "\n",
       "       ContractTime             Company                   Category  \\\n",
       "193962    permanent  MNM Associates LTD                    IT Jobs   \n",
       "166313     contract       Office Angels                 Admin Jobs   \n",
       "139918          NaN       Medical Alert  Healthcare & Nursing Jobs   \n",
       "\n",
       "                             SalaryRaw  SalaryNormalized        SourceName  \\\n",
       "193962  30000.00 - 35000.00 GBP Annual             32500      jobserve.com   \n",
       "166313                          8/hour             15360  cv-library.co.uk   \n",
       "139918                          14,285             14285   jobstoday.co.uk   \n",
       "\n",
       "        Log1pSalary  \n",
       "193962    10.389026  \n",
       "166313     9.639587  \n",
       "139918     9.567036  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./Train_rev1.csv\", index_col=None)\n",
    "data['Log1pSalary'] = np.log1p(data['SalaryNormalized']).astype('float32')\n",
    "\n",
    "text_columns = [\"Title\", \"FullDescription\"]\n",
    "categorical_columns = [\"Category\", \"Company\", \"LocationNormalized\", \"ContractType\", \"ContractTime\"]\n",
    "target_column = \"Log1pSalary\"\n",
    "data[categorical_columns] = data[categorical_columns].fillna('NaN') # cast nan to string\n",
    "\n",
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NLP part\n",
    "\n",
    "To even begin training our neural network, we're gonna need to preprocess the text features: tokenize it and build the token vocabularies.\n",
    "\n",
    "Since it is not an NLP course, we're gonna use simple built-in NLTK tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "0         Engineering Systems Analyst\n",
      "100000                   HR Assistant\n",
      "200000           Senior EC&I Engineer\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Before\")\n",
    "print(data[\"Title\"][::100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "for col in text_columns:\n",
    "    data[col] = data[col].apply(lambda l: ' '.join(tokenizer.tokenize(str(l).lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can assume that our text is a space-separated list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After\n",
      "0         engineering systems analyst\n",
      "100000                   hr assistant\n",
      "200000         senior ec & i engineer\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"After\")\n",
    "print(data[\"Title\"][::100000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all words are equally useful. Some of them are typos or rare words that are only present a few times. \n",
    "\n",
    "Let's see how many times is each word present in the data so that we can build a \"white list\" of known words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "token_counts = Counter()\n",
    "\n",
    "# Count how many times does each token occur in \"Title\" and \"FullDescription\"\n",
    "for title in data[\"Title\"]:\n",
    "    for word in title.split():\n",
    "        token_counts[word] += 1\n",
    "for description in data[\"FullDescription\"]:\n",
    "    for word in description.split():\n",
    "        token_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens : 202704\n",
      "('and', 2657388)\n",
      "('.', 2523216)\n",
      "(',', 2318606)\n",
      "('the', 2080994)\n",
      "('to', 2019884)\n",
      "...\n",
      "('stephanietraveltraderecruitmnt', 1)\n",
      "('ruabon', 1)\n",
      "('lowehays', 1)\n",
      "2657388\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "print(\"Total unique tokens :\", len(token_counts))\n",
    "print('\\n'.join(map(str, token_counts.most_common(n=5))))\n",
    "print('...')\n",
    "print('\\n'.join(map(str, token_counts.most_common()[-3:])))\n",
    "print(token_counts.most_common(1)[0][1])\n",
    "\n",
    "assert token_counts.most_common(1)[0][1] in  range(2600000, 2700000)\n",
    "assert len(token_counts) in range(200000, 210000)\n",
    "print('Correct!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Counts')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see how many words are there for each count\n",
    "\n",
    "_=plt.hist(list(token_counts.values()), range=[0, 10**4], bins=50, log=True)\n",
    "plt.xlabel(\"Counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.1__ Get a list of all tokens that occur at least 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "\n",
    "# tokens from token_counts keys that had at least min_count occurrences throughout the dataset\n",
    "tokens = [token for token,value in token_counts.items() if value > 10]\n",
    "\n",
    "# Add a special tokens for unknown and empty words\n",
    "UNK, PAD = \"UNK\", \"PAD\"\n",
    "tokens = [UNK, PAD] + tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens left: 32456\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens left:\", len(tokens))\n",
    "assert type(tokens)==list\n",
    "assert len(tokens) in range(32000,35000)\n",
    "assert 'me' in tokens\n",
    "assert UNK in tokens\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.2__ Build an inverse token index: a dictionary from token(string) to it's index in `tokens` (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = dict((tokens[i],i) for i in range(len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(token_to_id, dict)\n",
    "assert len(token_to_id) == len(tokens)\n",
    "for tok in tokens:\n",
    "    assert tokens[token_to_id[tok]] == tok\n",
    "\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's use the vocabulary you've built to map text lines into torch-digestible matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
    "\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    if isinstance(sequences[0], str):\n",
    "        sequences = list(map(str.split, sequences))\n",
    "        \n",
    "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
    "    \n",
    "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engineering systems analyst\n",
      "hr assistant\n",
      "senior ec & i engineer\n",
      "\n",
      "Matrix:\n",
      "[[   2    3    4    1    1]\n",
      " [ 996  176    1    1    1]\n",
      " [  18 3461  242   59    6]]\n"
     ]
    }
   ],
   "source": [
    "#### print(\"Lines:\")\n",
    "print('\\n'.join(data[\"Title\"][::100000].values), end='\\n\\n')\n",
    "print(\"Matrix:\")\n",
    "print(as_matrix(data[\"Title\"][::100000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's  encode the categirical data we have.\n",
    "\n",
    "As usual, we shall use one-hot encoding for simplicity. Kudos if you implement tf-idf, target averaging or pseudo-counter-based encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float32'>, separator='=', sort=True,\n",
       "        sparse=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# we only consider top-1k most frequent companies to minimize memory usage\n",
    "top_companies, top_counts = zip(*Counter(data['Company']).most_common(1000))\n",
    "recognized_companies = set(top_companies)\n",
    "data[\"Company\"] = data[\"Company\"].apply(lambda comp: comp if comp in recognized_companies else \"Other\")\n",
    "\n",
    "categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
    "categorical_vectorizer.fit(data[categorical_columns].apply(dict, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3768"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categorical_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data science part\n",
    "\n",
    "Once we've learned to tokenize the data, let's design a machine learning experiment.\n",
    "\n",
    "As before, we won't focus too much on validation, opting for a simple train-test split.\n",
    "\n",
    "__To be completely rigorous,__ we've comitted a small crime here: we used the whole data for tokenization and vocabulary building. A more strict way would be to do that part on training set only. You may want to do that and measure the magnitude of changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size =  220291\n",
      "Validation size =  24477\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Train size = \", len(data_train))\n",
    "print(\"Validation size = \", len(data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data, batch_size=None, replace=True, max_len=None):\n",
    "    \"\"\"\n",
    "    Creates a pytorch-friendly dict from the batch data.\n",
    "    :returns: a dict with {'title' : int64[batch, title_max_len]\n",
    "    \"\"\"\n",
    "    if batch_size is not None:\n",
    "        data = data.sample(batch_size, replace=replace)\n",
    "    \n",
    "    batch = {}\n",
    "    for col in text_columns:\n",
    "        batch[col] = as_matrix(data[col].values, max_len)\n",
    "    \n",
    "    batch['Categorical'] = categorical_vectorizer.transform(data[categorical_columns].apply(dict, axis=1))\n",
    "    \n",
    "    if target_column in data.columns:\n",
    "        batch[target_column] = data[target_column].values\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': array([[  18, 1117, 1121,   63,  203,  172,   64],\n",
       "        [ 105,  781, 1235,    1,    1,    1,    1],\n",
       "        [1594,    9, 1372,   23,  309,  298,    1]], dtype=int32),\n",
       " 'FullDescription': array([[   18,  1117,  1121,    63,   203,   172,    64, 16567,   704,\n",
       "           478],\n",
       "        [   80,  1164,   105,   781,     0,  1164,    74,  2152,  1164,\n",
       "            92],\n",
       "        [  360,  3020,  1164,  1594,     9,  1372,    23,  2152,  1164,\n",
       "           309]], dtype=int32),\n",
       " 'Categorical': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " 'Log1pSalary': array([ 9.64866 , 10.085851, 10.915107], dtype=float32)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_batch(data_train, 3, max_len=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's talk deep learning\n",
    "\n",
    "Out model consists of three branches:\n",
    "* Title encoder\n",
    "* Description encoder\n",
    "* Categorical features encoder\n",
    "\n",
    "We will then feed all 3 branches into one common network that predicts salary.\n",
    "\n",
    "![scheme](https://github.com/yandexdataschool/Practical_DL/raw/master/homework04/conv_salary_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, both text vectorizers shall use 1d convolutions, followed by global pooling over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class GlobalMaxPooling(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.max(dim=self.dim)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), out_size=64):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(n_tokens, 64, padding_idx=PAD_IX)\n",
    "        self.conv1 = nn.Conv1d(64, out_size, kernel_size=3, padding=1)\n",
    "        self.pool1 = GlobalMaxPooling()        \n",
    "        self.dense = nn.Linear(out_size, out_size)\n",
    "\n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        h = self.emb(text_ix)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        h = torch.transpose(h, 1, 2)\n",
    "        \n",
    "        # Apply the layers as defined above. Add some ReLUs before dense.\n",
    "        h = self.conv1(h)\n",
    "        h = self.pool1(h)\n",
    "        h = F.relu(self.dense(h))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine\n"
     ]
    }
   ],
   "source": [
    "title_encoder = TitleEncoder(out_size=64)\n",
    "\n",
    "dummy_x = Variable(torch.LongTensor(generate_batch(data_train, 3)['Title']))\n",
    "dummy_v = title_encoder(dummy_x)\n",
    "\n",
    "assert isinstance(dummy_v, Variable)\n",
    "assert tuple(dummy_v.shape) == (dummy_x.shape[0], 64)\n",
    "\n",
    "del title_encoder\n",
    "print(\"Seems fine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.1__ Create description encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an encoder for job descriptions.\n",
    "# Use any means you want so long as it's torch.nn.Module.\n",
    "class DescriptionEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), out_size=64):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(n_tokens, 64, padding_idx=PAD_IX)\n",
    "        self.conv1 = nn.Conv1d(64, out_size, kernel_size=5, padding=1)\n",
    "        self.pool1 = GlobalMaxPooling()        \n",
    "        self.dense = nn.Linear(out_size, out_size)\n",
    "        \n",
    "\n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        h = self.emb(text_ix)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        h = torch.transpose(h, 1, 2)\n",
    "        \n",
    "        # Apply the layers as defined above. Add some ReLUs before dense.\n",
    "        h = self.conv1(h)\n",
    "        h = self.pool1(h)\n",
    "        h = F.relu(self.dense(h))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine too\n"
     ]
    }
   ],
   "source": [
    "desc_encoder = DescriptionEncoder(out_size=64)\n",
    "\n",
    "dummy_x = Variable(torch.LongTensor(generate_batch(data_train, 3)['FullDescription']))\n",
    "dummy_v = desc_encoder(dummy_x)\n",
    "\n",
    "assert isinstance(dummy_v, Variable)\n",
    "assert tuple(dummy_v.shape) == (dummy_x.shape[0], 64)\n",
    "del desc_encoder\n",
    "print(\"Seems fine too\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Task 2.2__ Build one network ~~to rule them all~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalEncoder(nn.Module):\n",
    "    def __init__(self,n_cat_features,l_dims):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        l_dims = [n_cat_features]+l_dims\n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        for i in range(1,len(l_dims)):\n",
    "            self.dense_layers.append(nn.Linear(l_dims[i-1],l_dims[i]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        for l in self.dense_layers:\n",
    "            x = F.relu(l(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayers(nn.Module):\n",
    "    def __init__(self,n_concat,l_dims):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        l_dims = [n_concat]+l_dims\n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        for i in range(1,len(l_dims)):\n",
    "            self.dense_layers.append(nn.Linear(l_dims[i-1],l_dims[i]))\n",
    "        self.last_layer = nn.Linear(l_dims[-1],1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        for l in self.dense_layers:\n",
    "            x = F.relu(l(x))\n",
    "        return self.last_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This class does all the steps from (title, desc, categorical) features -> predicted target\n",
    "    It unites title & desc encoders you defined above as long as some layers for head and categorical branch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_)):\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        out_size_t, out_size_d = 64,64\n",
    "        self.title_encoder = TitleEncoder(out_size=out_size_t)\n",
    "        self.desc_encoder = DescriptionEncoder(out_size=out_size_d)\n",
    "        \n",
    "        # define layers for categorical features. A few dense layers would do.\n",
    "        l_dims = [300,100,20]\n",
    "        self.cat_encoder = CategoricalEncoder(n_cat_features,l_dims)\n",
    "        \n",
    "        # define \"output\" layers that process depend the three encoded vectors into answer\n",
    "        n_concat = out_size_t + out_size_d + l_dims[-1]\n",
    "        self.out_layers = OutputLayers(n_concat,[100,80,30])\n",
    "        \n",
    "        \n",
    "    def forward(self, title_ix, desc_ix, cat_features):\n",
    "        \"\"\"\n",
    "        :param title_ix: int32 Variable [batch, title_len], job titles encoded by as_matrix\n",
    "        :param desc_ix:  int32 Variable [batch, desc_len] , job descriptions encoded by as_matrix\n",
    "        :param cat_features: float32 Variable [batch, n_cat_features]\n",
    "        :returns: float32 Variable 1d [batch], predicted log1p-salary\n",
    "        \"\"\"\n",
    "        \n",
    "        # process each data source with it's respective encoder\n",
    "        title_h = self.title_encoder(title_ix)\n",
    "        desc_h = self.desc_encoder(desc_ix)\n",
    "        \n",
    "        # apply categorical encoder\n",
    "        cat_h = self.cat_encoder(cat_features)\n",
    "        \n",
    "        # concatenate all vectors together...\n",
    "        joint_h = torch.cat([title_h, desc_h, cat_h], dim=1)\n",
    "        \n",
    "        # ... and stack a few more layers at the top\n",
    "        output = self.out_layers(joint_h)\n",
    "        \n",
    "        # Note 1: do not forget to select first columns, [:, 0], to get to 1d outputs\n",
    "        # Note 2: please do not use output nonlinearities.\n",
    "        \n",
    "        return output[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullNetwork()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it on one batch\n",
    "\n",
    "batch = generate_batch(data_train, 32)\n",
    "\n",
    "title_ix = Variable(torch.LongTensor(batch[\"Title\"]))\n",
    "desc_ix = Variable(torch.LongTensor(batch[\"FullDescription\"]))\n",
    "cat_features = Variable(torch.FloatTensor(batch[\"Categorical\"]))\n",
    "reference = Variable(torch.FloatTensor(batch[target_column]))\n",
    "\n",
    "prediction = model(title_ix, desc_ix, cat_features)\n",
    "\n",
    "assert len(prediction.shape) == 1 and prediction.shape[0] == title_ix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(reference, prediction):\n",
    "    \"\"\"\n",
    "    Computes objective for minimization.\n",
    "    By deafult we minimize MSE, but you are encouraged to try mix up MSE, MAE, huber loss, etc.\n",
    "    \"\"\"\n",
    "    return torch.mean((prediction - reference) ** 2)\n",
    "\n",
    "def compute_mae(reference, prediction):\n",
    "    \"\"\" Compute MAE on actual salary, assuming your model outputs log1p(salary)\"\"\"\n",
    "    return torch.abs(torch.exp(reference - 1) - torch.exp(prediction - 1)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = compute_loss(reference, prediction)\n",
    "dummy_grads = torch.autograd.grad(loss, model.parameters(), retain_graph=True)\n",
    "for grad in dummy_grads:\n",
    "    assert grad is not None and not (grad == 0).all(), \"Some model parameters received zero grads. \" \\\n",
    "                                                       \"Double-check that your model uses all it's layers.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tnrange\n",
    "def iterate_minibatches(data, batch_size=32, max_len=None,\n",
    "                        max_batches=None, shuffle=True, verbose=False):\n",
    "    indices = np.arange(len(data))\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(indices)\n",
    "    if max_batches is not None:\n",
    "        indices = indices[: batch_size * max_batches]\n",
    "        \n",
    "    irange = tnrange if verbose else range\n",
    "    \n",
    "    for start in irange(0, len(indices), batch_size):\n",
    "        yield generate_batch(data.iloc[indices[start : start + batch_size]], max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "max_len = 100\n",
    "batch_size = 32\n",
    "batches_per_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullNetwork(\n",
       "  (title_encoder): TitleEncoder(\n",
       "    (emb): Embedding(32456, 64, padding_idx=1)\n",
       "    (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (pool1): GlobalMaxPooling()\n",
       "    (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (desc_encoder): DescriptionEncoder(\n",
       "    (emb): Embedding(32456, 64, padding_idx=1)\n",
       "    (conv1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(1,))\n",
       "    (pool1): GlobalMaxPooling()\n",
       "    (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (cat_encoder): CategoricalEncoder(\n",
       "    (dense_layers): ModuleList(\n",
       "      (0): Linear(in_features=3768, out_features=300, bias=True)\n",
       "      (1): Linear(in_features=300, out_features=100, bias=True)\n",
       "      (2): Linear(in_features=100, out_features=20, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (out_layers): OutputLayers(\n",
       "    (dense_layers): ModuleList(\n",
       "      (0): Linear(in_features=148, out_features=100, bias=True)\n",
       "      (1): Linear(in_features=100, out_features=80, bias=True)\n",
       "      (2): Linear(in_features=80, out_features=30, bias=True)\n",
       "    )\n",
       "    (last_layer): Linear(in_features=30, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4d4149aafd415bbb24fe06c67f6b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "\tLoss:\t18.42079\n",
      "\tMAE:\t58668.13160\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.65807\n",
      "\tMAE:\t7075.77153\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.51270\n",
      "\tMAE:\t6848.67526\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.45235\n",
      "\tMAE:\t5925.86479\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.41063\n",
      "\tMAE:\t5924.03076\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.39919\n",
      "\tMAE:\t5135.83520\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.31910\n",
      "\tMAE:\t5246.89538\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.31366\n",
      "\tMAE:\t5486.39947\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.28229\n",
      "\tMAE:\t4941.57447\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.26432\n",
      "\tMAE:\t4502.86168\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.24393\n",
      "\tMAE:\t4752.18316\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.23733\n",
      "\tMAE:\t4633.34705\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.23023\n",
      "\tMAE:\t4464.12187\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.24326\n",
      "\tMAE:\t4180.06699\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.19980\n",
      "\tMAE:\t4224.38859\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.21963\n",
      "\tMAE:\t4077.46270\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.19805\n",
      "\tMAE:\t4194.70801\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.18943\n",
      "\tMAE:\t3933.80442\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.18998\n",
      "\tMAE:\t4099.77887\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.17707\n",
      "\tMAE:\t3982.67648\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.19285\n",
      "\tMAE:\t4217.31774\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.22677\n",
      "\tMAE:\t5265.47278\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.17041\n",
      "\tMAE:\t3845.74095\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.16176\n",
      "\tMAE:\t3737.32102\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.15841\n",
      "\tMAE:\t3774.44534\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.17652\n",
      "\tMAE:\t3753.40257\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.17403\n",
      "\tMAE:\t3970.47798\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.18535\n",
      "\tMAE:\t4575.65490\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.17145\n",
      "\tMAE:\t3984.99530\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.14386\n",
      "\tMAE:\t3508.55559\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.14897\n",
      "\tMAE:\t3660.83942\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.14405\n",
      "\tMAE:\t3489.14313\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.14960\n",
      "\tMAE:\t3731.94568\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.14186\n",
      "\tMAE:\t3705.33715\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.14517\n",
      "\tMAE:\t3664.83131\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.13875\n",
      "\tMAE:\t3415.77731\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.13338\n",
      "\tMAE:\t3373.12666\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.13117\n",
      "\tMAE:\t3450.34052\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.13693\n",
      "\tMAE:\t3502.32991\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.12950\n",
      "\tMAE:\t3338.99285\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.14052\n",
      "\tMAE:\t3496.49151\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.14606\n",
      "\tMAE:\t3991.71373\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.13975\n",
      "\tMAE:\t3640.00974\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.13474\n",
      "\tMAE:\t3646.16199\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.12846\n",
      "\tMAE:\t3596.24285\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.12564\n",
      "\tMAE:\t3253.50242\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.12322\n",
      "\tMAE:\t3374.04539\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.11630\n",
      "\tMAE:\t3252.62403\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.12591\n",
      "\tMAE:\t3351.04981\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.14587\n",
      "\tMAE:\t3494.20821\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.14044\n",
      "\tMAE:\t3647.16418\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.13186\n",
      "\tMAE:\t3699.82900\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.12127\n",
      "\tMAE:\t3360.84329\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.14551\n",
      "\tMAE:\t3491.61849\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.12419\n",
      "\tMAE:\t3345.19123\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10845\n",
      "\tMAE:\t3088.65106\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.11681\n",
      "\tMAE:\t3296.77008\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10854\n",
      "\tMAE:\t3098.07433\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.11833\n",
      "\tMAE:\t3295.58006\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10948\n",
      "\tMAE:\t3193.60885\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.12913\n",
      "\tMAE:\t3504.56640\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.17044\n",
      "\tMAE:\t3746.30373\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.11901\n",
      "\tMAE:\t3395.71998\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.11066\n",
      "\tMAE:\t3145.66488\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.13902\n",
      "\tMAE:\t3699.50179\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10715\n",
      "\tMAE:\t3087.22338\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.11014\n",
      "\tMAE:\t3128.58813\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10600\n",
      "\tMAE:\t3094.43398\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.12096\n",
      "\tMAE:\t3298.76866\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10674\n",
      "\tMAE:\t3100.16909\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.11305\n",
      "\tMAE:\t3201.80960\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10869\n",
      "\tMAE:\t3279.95677\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.12459\n",
      "\tMAE:\t3382.28008\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10403\n",
      "\tMAE:\t3050.66309\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.10601\n",
      "\tMAE:\t3132.77072\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.11364\n",
      "\tMAE:\t3142.10609\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.10590\n",
      "\tMAE:\t3131.98045\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.16279\n",
      "\tMAE:\t4443.54081\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.12077\n",
      "\tMAE:\t3367.62764\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.11141\n",
      "\tMAE:\t3319.28596\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.10604\n",
      "\tMAE:\t3166.19676\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.11123\n",
      "\tMAE:\t3141.40492\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.11840\n",
      "\tMAE:\t3281.80611\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10369\n",
      "\tMAE:\t3162.11291\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.10920\n",
      "\tMAE:\t3222.44388\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.09960\n",
      "\tMAE:\t3059.23923\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.12355\n",
      "\tMAE:\t3402.67953\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.11293\n",
      "\tMAE:\t3344.56160\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.11173\n",
      "\tMAE:\t3258.89665\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10206\n",
      "\tMAE:\t3131.81321\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.10915\n",
      "\tMAE:\t3184.72136\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.09489\n",
      "\tMAE:\t2927.70700\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09778\n",
      "\tMAE:\t3019.07640\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.09486\n",
      "\tMAE:\t2908.60845\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09293\n",
      "\tMAE:\t2965.91985\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.09354\n",
      "\tMAE:\t2923.49072\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09643\n",
      "\tMAE:\t3012.11841\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.12143\n",
      "\tMAE:\t3200.20167\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.10385\n",
      "\tMAE:\t3183.23036\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.09331\n",
      "\tMAE:\t2889.84971\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.10974\n",
      "\tMAE:\t3259.66000\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.09302\n",
      "\tMAE:\t2876.59241\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09161\n",
      "\tMAE:\t2931.32085\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.14058\n",
      "\tMAE:\t3442.96728\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.11207\n",
      "\tMAE:\t3233.65387\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.11262\n",
      "\tMAE:\t3380.99812\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09561\n",
      "\tMAE:\t2974.43010\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.09546\n",
      "\tMAE:\t2961.37817\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.11121\n",
      "\tMAE:\t3308.79222\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10045\n",
      "\tMAE:\t2985.43169\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.11649\n",
      "\tMAE:\t3333.48868\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10031\n",
      "\tMAE:\t3119.33589\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.10251\n",
      "\tMAE:\t3094.23267\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10369\n",
      "\tMAE:\t3012.65112\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09041\n",
      "\tMAE:\t2908.81137\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10048\n",
      "\tMAE:\t3130.08898\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09047\n",
      "\tMAE:\t2913.70122\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.09628\n",
      "\tMAE:\t3043.54928\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09877\n",
      "\tMAE:\t3018.84683\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.09263\n",
      "\tMAE:\t2952.15800\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.10221\n",
      "\tMAE:\t3054.35006\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.09056\n",
      "\tMAE:\t2838.08842\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09861\n",
      "\tMAE:\t3054.04915\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08950\n",
      "\tMAE:\t2882.56209\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09224\n",
      "\tMAE:\t2920.87059\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08901\n",
      "\tMAE:\t2831.67652\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.10530\n",
      "\tMAE:\t3165.19997\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.15672\n",
      "\tMAE:\t4413.16890\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09550\n",
      "\tMAE:\t3041.36658\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08773\n",
      "\tMAE:\t2856.96300\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.10227\n",
      "\tMAE:\t3157.50348\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.09159\n",
      "\tMAE:\t2931.01363\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09190\n",
      "\tMAE:\t2889.28402\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08464\n",
      "\tMAE:\t2781.08415\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.08370\n",
      "\tMAE:\t2748.77143\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08669\n",
      "\tMAE:\t2771.51052\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.08632\n",
      "\tMAE:\t2842.75485\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.09358\n",
      "\tMAE:\t3004.03682\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.08521\n",
      "\tMAE:\t2858.82699\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08514\n",
      "\tMAE:\t2805.05621\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.08214\n",
      "\tMAE:\t2733.11235\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.09570\n",
      "\tMAE:\t2904.21882\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09855\n",
      "\tMAE:\t2990.27043\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08542\n",
      "\tMAE:\t2791.99526\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.08452\n",
      "\tMAE:\t2776.07535\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08419\n",
      "\tMAE:\t2819.16664\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09655\n",
      "\tMAE:\t2977.74638\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10170\n",
      "\tMAE:\t3222.50344\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09137\n",
      "\tMAE:\t3018.57261\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10019\n",
      "\tMAE:\t2960.49050\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09810\n",
      "\tMAE:\t3106.41016\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.09526\n",
      "\tMAE:\t2884.73775\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.08901\n",
      "\tMAE:\t2867.87966\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.11950\n",
      "\tMAE:\t3244.69852\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09085\n",
      "\tMAE:\t2931.06258\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.09589\n",
      "\tMAE:\t3112.18333\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.08594\n",
      "\tMAE:\t2802.62928\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.13874\n",
      "\tMAE:\t3465.02731\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.08917\n",
      "\tMAE:\t2913.10531\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08413\n",
      "\tMAE:\t2838.87702\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09122\n",
      "\tMAE:\t2953.06605\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08863\n",
      "\tMAE:\t2928.06972\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.08839\n",
      "\tMAE:\t2876.19583\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08257\n",
      "\tMAE:\t2709.29743\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.07993\n",
      "\tMAE:\t2815.96307\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08234\n",
      "\tMAE:\t2736.19616\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.08542\n",
      "\tMAE:\t2822.32513\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.09060\n",
      "\tMAE:\t2849.65563\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.08449\n",
      "\tMAE:\t2814.01588\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08886\n",
      "\tMAE:\t2811.31843\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.08101\n",
      "\tMAE:\t2838.02516\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10925\n",
      "\tMAE:\t3083.29760\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.10946\n",
      "\tMAE:\t3203.68936\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10158\n",
      "\tMAE:\t2974.71041\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.08060\n",
      "\tMAE:\t2766.86417\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08006\n",
      "\tMAE:\t2675.90044\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09970\n",
      "\tMAE:\t3091.00796\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08172\n",
      "\tMAE:\t2692.11379\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t0.09216\n",
      "\tMAE:\t3022.95020\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.09727\n",
      "\tMAE:\t3182.01569\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09365\n",
      "\tMAE:\t2971.20906\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.11297\n",
      "\tMAE:\t3504.09428\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.08672\n",
      "\tMAE:\t2872.37007\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.09935\n",
      "\tMAE:\t2971.80153\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.09668\n",
      "\tMAE:\t3037.74942\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08711\n",
      "\tMAE:\t2873.15295\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.08470\n",
      "\tMAE:\t2744.53578\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.11047\n",
      "\tMAE:\t3420.22981\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.10253\n",
      "\tMAE:\t3112.45111\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08250\n",
      "\tMAE:\t2780.13260\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.08116\n",
      "\tMAE:\t2733.90247\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.07934\n",
      "\tMAE:\t2695.79687\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.08015\n",
      "\tMAE:\t2723.93637\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.10554\n",
      "\tMAE:\t3341.56939\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.08264\n",
      "\tMAE:\t2749.67072\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.07879\n",
      "\tMAE:\t2687.63505\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.07712\n",
      "\tMAE:\t2679.51910\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08819\n",
      "\tMAE:\t2990.15232\n",
      "\n",
      "\n",
      "\n",
      "Training:\n",
      "\tLoss:\t0.07224\n",
      "\tMAE:\t2578.41708\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n",
      "\tLoss:\t0.08410\n",
      "\tMAE:\t2879.63493\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_train = []\n",
    "history_val = []\n",
    " \n",
    "for epoch_i in tnrange(num_epochs):\n",
    "    history_train = []\n",
    "    history_val = []\n",
    " \n",
    "    print(\"Training:\")\n",
    "    train_loss = train_mae = train_batches = 0    \n",
    "    model.train(True)\n",
    "    \n",
    "    for batch in iterate_minibatches(data_train, max_batches=batches_per_epoch):\n",
    "\n",
    "        title_ix = torch.tensor(batch[\"Title\"],dtype=torch.long,device=device)\n",
    "        desc_ix = torch.tensor(batch[\"FullDescription\"],dtype=torch.long,device=device)\n",
    "        cat_features = torch.tensor(batch[\"Categorical\"],dtype=torch.float32,device=device)\n",
    "        reference = torch.tensor(batch[target_column],dtype=torch.float32,device=device)\n",
    "\n",
    "        prediction = model(title_ix, desc_ix, cat_features)\n",
    "\n",
    "        loss = compute_loss(reference, prediction)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        train_loss += loss.data.cpu().numpy()\n",
    "        train_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "        train_batches += 1\n",
    "    \n",
    "    print(\"\\tLoss:\\t%.5f\" % (train_loss / train_batches))\n",
    "    print(\"\\tMAE:\\t%.5f\" % (train_mae / train_batches))\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    history_train.append(train_loss / train_batches)\n",
    "    \n",
    "    print(\"Validation:\")\n",
    "    val_loss = val_mae = val_batches = 0\n",
    "    model.train(False)\n",
    "    \n",
    "    for batch in iterate_minibatches(data_val, max_batches=batches_per_epoch, shuffle=False):\n",
    "        title_ix = torch.tensor(batch[\"Title\"],dtype=torch.long,device=device)\n",
    "        desc_ix = torch.tensor(batch[\"FullDescription\"],dtype=torch.long,device=device)\n",
    "        cat_features = torch.tensor(batch[\"Categorical\"],dtype=torch.float32,device=device)\n",
    "        reference = torch.tensor(batch[target_column],dtype=torch.float32,device=device)\n",
    "\n",
    "\n",
    "        prediction = model(title_ix, desc_ix, cat_features)\n",
    "        loss = compute_loss(reference, prediction)\n",
    "\n",
    "        val_loss += loss.data.cpu().numpy()\n",
    "        val_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "        val_batches += 1\n",
    "        \n",
    "    print(\"\\tLoss:\\t%.5f\" % (val_loss / val_batches))\n",
    "    print(\"\\tMAE:\\t%.5f\" % (val_mae / val_batches))\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    history_val.append(val_loss / val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type FullNetwork. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/lib/python3.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TitleEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/lib/python3.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type GlobalMaxPooling. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/lib/python3.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type DescriptionEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/lib/python3.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type CategoricalEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/lib/python3.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type OutputLayers. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model,\"models/conv_1d.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval:\n",
      "\tLoss:\t0.08744\n",
      "\tMAE:\t2937.63382\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Final eval:\")\n",
    "val_loss = val_mae = val_batches = 0\n",
    "\n",
    "for batch in iterate_minibatches(data_val, shuffle=False):\n",
    "    title_ix = torch.tensor(batch[\"Title\"],dtype=torch.long,device=device)\n",
    "    desc_ix = torch.tensor(batch[\"FullDescription\"],dtype=torch.long,device=device)\n",
    "    cat_features = torch.tensor(batch[\"Categorical\"],dtype=torch.float32,device=device)\n",
    "    reference = torch.tensor(batch[target_column],dtype=torch.float32,device=device)\n",
    "\n",
    "    prediction = model(title_ix, desc_ix, cat_features)\n",
    "    loss = compute_loss(reference, prediction)\n",
    "\n",
    "    val_loss += loss.data.cpu().numpy()\n",
    "    val_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "    val_batches += 1\n",
    "\n",
    "print(\"\\tLoss:\\t%.5f\" % (val_loss / val_batches))\n",
    "print(\"\\tMAE:\\t%.5f\" % (val_mae / val_batches))\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Actually make it work\n",
    "\n",
    "Your main task is to use some of the tricks you've learned on the network and analyze if you can improve __validation MAE__.\n",
    "\n",
    "Try __at least 3 options__ from the list below for a passing grade. If you're into \n",
    "\n",
    "#### A) CNN architecture\n",
    "\n",
    "All the tricks you know about dense and convolutional neural networks apply here as well.\n",
    "* Dropout. Nuff said.\n",
    "* Batch Norm. This time it's `nn.BatchNorm1d`\n",
    "* Parallel convolution layers. The idea is that you apply several nn.Conv1d to the same embeddings and concatenate output channels.\n",
    "* More layers, more neurons, ya know...\n",
    "\n",
    "\n",
    "#### B) Play with pooling\n",
    "\n",
    "There's more than one way to do max pooling:\n",
    "* Max over time - our `GlobalMaxPooling`\n",
    "* Average over time (excluding PAD)\n",
    "* Softmax-pooling:\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot {{e ^ {h_{i, t}}} \\over \\sum_\\tau e ^ {h_{j, \\tau}} } }$$\n",
    "\n",
    "* Attentive pooling\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot Attn(h_t)}$$\n",
    "\n",
    ", where $$ Attn(h_t) = {{e ^ {NN_{attn}(h_t)}} \\over \\sum_\\tau e ^ {NN_{attn}(h_\\tau)}}  $$\n",
    "and $NN_{attn}$ is a small neural network\n",
    "\n",
    "\n",
    "The optimal score is usually achieved by concatenating several different poolings, including several attentive pooling with different $NN_{attn}$\n",
    "\n",
    "#### C) Fun with embeddings\n",
    "\n",
    "It's not always a good idea to train embeddings from scratch. Here's a few tricks:\n",
    "\n",
    "* Use a pre-trained word2vec from [here](http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/) or [here](http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/).\n",
    "* Start with pre-trained embeddings, then fine-tune them with gradient descent\n",
    "* Use the same embedding matrix in title and desc vectorizer\n",
    "\n",
    "#### D) Going recurrent\n",
    "\n",
    "We've already learned that recurrent networks can do cool stuff in sequence modelling. Turns out, they're not useless for classification as well. With some tricks of course..\n",
    "\n",
    "* Like convolutional layers, LSTM should be pooled into a fixed-size vector with some of the poolings.\n",
    "  * Please bear in mind that while convolution uses [batch, units, time] dim order, \n",
    "    recurrent units are built for [batch, time, unit]. You may need to `torch.transpose`.\n",
    "\n",
    "* Since you know all the text in advance, use bidirectional RNN\n",
    "  * Run one LSTM from left to right\n",
    "  * Run another in parallel from right to left \n",
    "  * Concatenate their output sequences along unit axis (dim=-1)\n",
    "\n",
    "* It might be good idea to mix convolutions and recurrent layers differently for title and description\n",
    "\n",
    "\n",
    "#### E) Optimizing seriously\n",
    "\n",
    "* You don't necessarily need 100 epochs. Use early stopping. If you've never done this before, take a look at [keras](https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L461) for inspiration.\n",
    "  * In short, train until you notice that validation\n",
    "  * Maintain the best-on-validation snapshot via `model.state_dict`\n",
    "  * Plotting learning curves is usually a good idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A short report\n",
    "\n",
    "Please tell us what you did and how did it work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Improve CNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Added parallel conv layers and batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalEncoder(nn.Module):\n",
    "    def __init__(self,n_cat_features,l_dims):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        l_dims = [n_cat_features]+l_dims\n",
    "        self.dense = nn.ModuleList()\n",
    "        self.bn = nn.ModuleList()\n",
    "        for i in range(1,len(l_dims)):\n",
    "            self.dense.append(nn.Linear(l_dims[i-1],l_dims[i]))\n",
    "            self.bn.append(nn.BatchNorm1d(l_dims[i]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        for i in range(len(self.dense)):\n",
    "            x = self.dense[i](x)\n",
    "            x = self.bn[i](x)\n",
    "            x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayers(nn.Module):\n",
    "    def __init__(self,n_concat,l_dims):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        l_dims = [n_concat]+l_dims\n",
    "        self.dense = nn.ModuleList()\n",
    "        self.bn = nn.ModuleList()\n",
    "        for i in range(1,len(l_dims)):\n",
    "            self.dense.append(nn.Linear(l_dims[i-1],l_dims[i]))\n",
    "            self.bn.append(nn.BatchNorm1d(l_dims[i]))\n",
    "        self.last_layer = nn.Linear(l_dims[-1],1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        for i in range(len(self.dense)):\n",
    "            x = self.dense[i](x)\n",
    "            x = self.bn[i](x)\n",
    "            x = F.relu(x)\n",
    "        return self.last_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), sizes=[48,48]):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(n_tokens, 64, padding_idx=PAD_IX)\n",
    "        self.conv1 = nn.Conv1d(64, sizes[0], kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, sizes[1], kernel_size=5)\n",
    "        self.bn = nn.BatchNorm1d(sum(sizes))\n",
    "        self.pool1 = GlobalMaxPooling()        \n",
    "        self.dense = nn.Linear(sum(sizes), sum(sizes))\n",
    "\n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, size1+size2]\n",
    "        \"\"\"\n",
    "        h = self.emb(text_ix)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        h = torch.transpose(h, 1, 2)\n",
    "        \n",
    "        # Apply the layers as defined above. Add some ReLUs before dense.\n",
    "        h1 = self.conv1(h)\n",
    "        h1 = self.pool1(h1)\n",
    "        h2 = self.conv2(h)\n",
    "        h2 = self.pool1(h2)\n",
    "        h = torch.cat([h1,h2], dim=1)\n",
    "        h = self.bn(h)\n",
    "        h = F.relu(self.dense(h)) \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an encoder for job descriptions.\n",
    "# Use any means you want so long as it's torch.nn.Module.\n",
    "class DescriptionEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), sizes=[48,48,48]):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(n_tokens, 64, padding_idx=PAD_IX)\n",
    "        self.conv1 = nn.Conv1d(64, sizes[0], kernel_size=5, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, sizes[1], kernel_size=7)\n",
    "        self.conv3 = nn.Conv1d(64, sizes[2], kernel_size=9)\n",
    "        self.pool1 = GlobalMaxPooling()    \n",
    "        self.bn = nn.BatchNorm1d(sum(sizes))\n",
    "        self.dense = nn.Linear(sum(sizes), sum(sizes))\n",
    "        \n",
    "\n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        h = self.emb(text_ix)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        h = torch.transpose(h, 1, 2)\n",
    "        \n",
    "        # Apply the layers as defined above. Add some ReLUs before dense.\n",
    "        h1 = self.conv1(h)\n",
    "        h1 = self.pool1(h1)\n",
    "        h2 = self.conv2(h)\n",
    "        h2 = self.pool1(h2)\n",
    "        h3 = self.conv3(h)\n",
    "        h3 = self.pool1(h3)\n",
    "        h = torch.cat([h1,h2,h3], dim=1)\n",
    "        h = self.bn(h)\n",
    "        h = F.relu(self.dense(h))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This class does all the steps from (title, desc, categorical) features -> predicted target\n",
    "    It unites title & desc encoders you defined above as long as some layers for head and categorical branch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_),\n",
    "                t_sizes = [48,48], d_sizes = [48,48,48]):\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.title_encoder = TitleEncoder(sizes=t_sizes)\n",
    "        self.desc_encoder = DescriptionEncoder(sizes=d_sizes)\n",
    "        \n",
    "        # define layers for categorical features. A few dense layers would do.\n",
    "        l_dims = [300,100,20]\n",
    "        self.cat_encoder = CategoricalEncoder(n_cat_features,l_dims)\n",
    "        \n",
    "        # define \"output\" layers that process depend the three encoded vectors into answer\n",
    "        n_concat = sum(t_sizes) + sum(d_sizes) + l_dims[-1]\n",
    "        self.out_layers = OutputLayers(n_concat,[100,80,30])\n",
    "        \n",
    "        \n",
    "    def forward(self, title_ix, desc_ix, cat_features):\n",
    "        \"\"\"\n",
    "        :param title_ix: int32 Variable [batch, title_len], job titles encoded by as_matrix\n",
    "        :param desc_ix:  int32 Variable [batch, desc_len] , job descriptions encoded by as_matrix\n",
    "        :param cat_features: float32 Variable [batch, n_cat_features]\n",
    "        :returns: float32 Variable 1d [batch], predicted log1p-salary\n",
    "        \"\"\"\n",
    "        \n",
    "        # process each data source with it's respective encoder\n",
    "        title_h = self.title_encoder(title_ix)\n",
    "        desc_h = self.desc_encoder(desc_ix)\n",
    "        \n",
    "        # apply categorical encoder\n",
    "        cat_h = self.cat_encoder(cat_features)\n",
    "        \n",
    "        # concatenate all vectors together...\n",
    "        joint_h = torch.cat([title_h, desc_h, cat_h], dim=1)\n",
    "        \n",
    "        # ... and stack a few more layers at the top\n",
    "        output = self.out_layers(joint_h)\n",
    "        \n",
    "        # Note 1: do not forget to select first columns, [:, 0], to get to 1d outputs\n",
    "        # Note 2: please do not use output nonlinearities.\n",
    "        \n",
    "        return output[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(model, num_epochs, opt, batch_size=32, batches_per_epoch=100):\n",
    "    history_train = []\n",
    "    history_val = []\n",
    "    print(\"\\n\\n\")\n",
    "    for epoch_i in tnrange(num_epochs):\n",
    "        print(\"Epoch \"+str(epoch_i))\n",
    "        print(\"\\tTraining:\")\n",
    "        train_loss = train_mae = train_batches = 0\n",
    "        model.train(True)\n",
    "\n",
    "        for batch in iterate_minibatches(data_train, batch_size=batch_size, max_batches=batches_per_epoch):\n",
    "\n",
    "            title_ix = torch.tensor(batch[\"Title\"],dtype=torch.long,device=device)\n",
    "            desc_ix = torch.tensor(batch[\"FullDescription\"],dtype=torch.long,device=device)\n",
    "            cat_features = torch.tensor(batch[\"Categorical\"],dtype=torch.float32,device=device)\n",
    "            reference = torch.tensor(batch[target_column],dtype=torch.float32,device=device)\n",
    "            prediction = model(title_ix, desc_ix, cat_features)\n",
    "            loss = compute_loss(reference, prediction)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            train_loss += loss.data.cpu().numpy()\n",
    "            train_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "            train_batches += 1\n",
    "\n",
    "        print(\"\\tLoss:\\t%.5f\" % (train_loss / train_batches))\n",
    "        print(\"\\tMAE:\\t%.5f\" % (train_mae / train_batches))\n",
    "\n",
    "        history_train.append(train_loss / train_batches)\n",
    "\n",
    "        print(\"\\tValidation:\")\n",
    "        val_loss = val_mae = val_batches = 0\n",
    "        model.train(False)\n",
    "\n",
    "        for batch in iterate_minibatches(data_val, batch_size=batch_size, max_batches=batches_per_epoch, shuffle=False):\n",
    "            title_ix = torch.tensor(batch[\"Title\"],dtype=torch.long,device=device)\n",
    "            desc_ix = torch.tensor(batch[\"FullDescription\"],dtype=torch.long,device=device)\n",
    "            cat_features = torch.tensor(batch[\"Categorical\"],dtype=torch.float32,device=device)\n",
    "            reference = torch.tensor(batch[target_column],dtype=torch.float32,device=device)\n",
    "            prediction = model(title_ix, desc_ix, cat_features)\n",
    "            loss = compute_loss(reference, prediction)\n",
    "\n",
    "            val_loss += loss.data.cpu().numpy()\n",
    "            val_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "            val_batches += 1\n",
    "\n",
    "        print(\"\\tLoss:\\t%.5f\" % (val_loss / val_batches))\n",
    "        print(\"\\tMAE:\\t%.5f\" % (val_mae / val_batches))\n",
    "        print('\\n')\n",
    "\n",
    "        history_val.append(val_loss / val_batches)\n",
    "    return history_train, history_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(model):\n",
    "    print(\"Final eval:\")\n",
    "    val_loss = val_mae = val_batches = 0\n",
    "\n",
    "    for batch in iterate_minibatches(data_val, shuffle=False):\n",
    "        title_ix = torch.tensor(batch[\"Title\"],dtype=torch.long,device=device)\n",
    "        desc_ix = torch.tensor(batch[\"FullDescription\"],dtype=torch.long,device=device)\n",
    "        cat_features = torch.tensor(batch[\"Categorical\"],dtype=torch.float32,device=device)\n",
    "        reference = torch.tensor(batch[target_column],dtype=torch.float32,device=device)\n",
    "\n",
    "        prediction = model(title_ix, desc_ix, cat_features)\n",
    "        loss = compute_loss(reference, prediction)\n",
    "\n",
    "        val_loss += loss.data.cpu().numpy()\n",
    "        val_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "        val_batches += 1\n",
    "\n",
    "    print(\"\\tLoss:\\t%.5f\" % (val_loss / val_batches))\n",
    "    print(\"\\tMAE:\\t%.5f\" % (val_mae / val_batches))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullNetwork()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullNetwork(\n",
       "  (title_encoder): TitleEncoder(\n",
       "    (emb): Embedding(32456, 64, padding_idx=1)\n",
       "    (conv1): Conv1d(64, 48, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(64, 48, kernel_size=(5,), stride=(1,))\n",
       "    (bn): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pool1): GlobalMaxPooling()\n",
       "    (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "  )\n",
       "  (desc_encoder): DescriptionEncoder(\n",
       "    (emb): Embedding(32456, 64, padding_idx=1)\n",
       "    (conv1): Conv1d(64, 48, kernel_size=(5,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(64, 48, kernel_size=(7,), stride=(1,))\n",
       "    (conv3): Conv1d(64, 48, kernel_size=(9,), stride=(1,))\n",
       "    (pool1): GlobalMaxPooling()\n",
       "    (bn): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dense): Linear(in_features=144, out_features=144, bias=True)\n",
       "  )\n",
       "  (cat_encoder): CategoricalEncoder(\n",
       "    (dense): ModuleList(\n",
       "      (0): Linear(in_features=3768, out_features=300, bias=True)\n",
       "      (1): Linear(in_features=300, out_features=100, bias=True)\n",
       "      (2): Linear(in_features=100, out_features=20, bias=True)\n",
       "    )\n",
       "    (bn): ModuleList(\n",
       "      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (out_layers): OutputLayers(\n",
       "    (dense): ModuleList(\n",
       "      (0): Linear(in_features=260, out_features=100, bias=True)\n",
       "      (1): Linear(in_features=100, out_features=80, bias=True)\n",
       "      (2): Linear(in_features=80, out_features=30, bias=True)\n",
       "    )\n",
       "    (bn): ModuleList(\n",
       "      (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (last_layer): Linear(in_features=30, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494dfa6d755b4d1f8473323edfc73b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tTraining:\n",
      "\tLoss:\t93.66297\n",
      "\tMAE:\t12552.49212\n",
      "\tValidation:\n",
      "\tLoss:\t82.13053\n",
      "\tMAE:\t12605.37854\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "\tTraining:\n",
      "\tLoss:\t65.62862\n",
      "\tMAE:\t12416.66029\n",
      "\tValidation:\n",
      "\tLoss:\t47.57623\n",
      "\tMAE:\t12594.81192\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "\tTraining:\n",
      "\tLoss:\t31.22666\n",
      "\tMAE:\t12442.96698\n",
      "\tValidation:\n",
      "\tLoss:\t17.62059\n",
      "\tMAE:\t12428.68049\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "\tTraining:\n",
      "\tLoss:\t8.48709\n",
      "\tMAE:\t11636.52795\n",
      "\tValidation:\n",
      "\tLoss:\t2.84160\n",
      "\tMAE:\t10283.60507\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "\tTraining:\n",
      "\tLoss:\t1.24303\n",
      "\tMAE:\t7808.11684\n",
      "\tValidation:\n",
      "\tLoss:\t0.53096\n",
      "\tMAE:\t5941.21310\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "\tTraining:\n",
      "\tLoss:\t0.26538\n",
      "\tMAE:\t4601.52495\n",
      "\tValidation:\n",
      "\tLoss:\t0.21873\n",
      "\tMAE:\t4162.12118\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "\tTraining:\n",
      "\tLoss:\t0.16547\n",
      "\tMAE:\t3916.46133\n",
      "\tValidation:\n",
      "\tLoss:\t0.13648\n",
      "\tMAE:\t3551.92083\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "\tTraining:\n",
      "\tLoss:\t0.15882\n",
      "\tMAE:\t3831.24543\n",
      "\tValidation:\n",
      "\tLoss:\t0.14094\n",
      "\tMAE:\t3641.21316\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "\tTraining:\n",
      "\tLoss:\t0.14119\n",
      "\tMAE:\t3638.55699\n",
      "\tValidation:\n",
      "\tLoss:\t0.12736\n",
      "\tMAE:\t3400.72133\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "\tTraining:\n",
      "\tLoss:\t0.14010\n",
      "\tMAE:\t3541.74085\n",
      "\tValidation:\n",
      "\tLoss:\t0.13511\n",
      "\tMAE:\t3568.28437\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "\tTraining:\n",
      "\tLoss:\t0.13948\n",
      "\tMAE:\t3597.19585\n",
      "\tValidation:\n",
      "\tLoss:\t0.12046\n",
      "\tMAE:\t3337.65352\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "\tTraining:\n",
      "\tLoss:\t0.12713\n",
      "\tMAE:\t3419.98598\n",
      "\tValidation:\n",
      "\tLoss:\t0.12053\n",
      "\tMAE:\t3319.85200\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "\tTraining:\n",
      "\tLoss:\t0.11833\n",
      "\tMAE:\t3308.76421\n",
      "\tValidation:\n",
      "\tLoss:\t0.12399\n",
      "\tMAE:\t3386.67027\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "\tTraining:\n",
      "\tLoss:\t0.11628\n",
      "\tMAE:\t3306.84948\n",
      "\tValidation:\n",
      "\tLoss:\t0.10450\n",
      "\tMAE:\t3132.59621\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "\tTraining:\n",
      "\tLoss:\t0.12185\n",
      "\tMAE:\t3359.76537\n",
      "\tValidation:\n",
      "\tLoss:\t0.10673\n",
      "\tMAE:\t3153.30859\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "\tTraining:\n",
      "\tLoss:\t0.10683\n",
      "\tMAE:\t3167.61972\n",
      "\tValidation:\n",
      "\tLoss:\t0.10226\n",
      "\tMAE:\t3072.58791\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "\tTraining:\n",
      "\tLoss:\t0.10266\n",
      "\tMAE:\t3107.27900\n",
      "\tValidation:\n",
      "\tLoss:\t0.10991\n",
      "\tMAE:\t3168.62516\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "\tTraining:\n",
      "\tLoss:\t0.10496\n",
      "\tMAE:\t3137.19959\n",
      "\tValidation:\n",
      "\tLoss:\t0.11122\n",
      "\tMAE:\t3202.86878\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "\tTraining:\n",
      "\tLoss:\t0.10170\n",
      "\tMAE:\t3073.41960\n",
      "\tValidation:\n",
      "\tLoss:\t0.09698\n",
      "\tMAE:\t2979.35826\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "\tTraining:\n",
      "\tLoss:\t0.10060\n",
      "\tMAE:\t3074.97524\n",
      "\tValidation:\n",
      "\tLoss:\t0.09155\n",
      "\tMAE:\t2941.36288\n",
      "\n",
      "\n",
      "Epoch 20\n",
      "\tTraining:\n",
      "\tLoss:\t0.10006\n",
      "\tMAE:\t3006.15562\n",
      "\tValidation:\n",
      "\tLoss:\t0.09108\n",
      "\tMAE:\t2912.20375\n",
      "\n",
      "\n",
      "Epoch 21\n",
      "\tTraining:\n",
      "\tLoss:\t0.09937\n",
      "\tMAE:\t3074.96755\n",
      "\tValidation:\n",
      "\tLoss:\t0.09640\n",
      "\tMAE:\t3002.77639\n",
      "\n",
      "\n",
      "Epoch 22\n",
      "\tTraining:\n",
      "\tLoss:\t0.09719\n",
      "\tMAE:\t2953.63394\n",
      "\tValidation:\n",
      "\tLoss:\t0.08889\n",
      "\tMAE:\t2870.15267\n",
      "\n",
      "\n",
      "Epoch 23\n",
      "\tTraining:\n",
      "\tLoss:\t0.09248\n",
      "\tMAE:\t2894.14129\n",
      "\tValidation:\n",
      "\tLoss:\t0.09104\n",
      "\tMAE:\t2897.31620\n",
      "\n",
      "\n",
      "Epoch 24\n",
      "\tTraining:\n",
      "\tLoss:\t0.09361\n",
      "\tMAE:\t2958.27384\n",
      "\tValidation:\n",
      "\tLoss:\t0.09331\n",
      "\tMAE:\t2954.98899\n",
      "\n",
      "\n",
      "Epoch 25\n",
      "\tTraining:\n",
      "\tLoss:\t0.08785\n",
      "\tMAE:\t2834.46465\n",
      "\tValidation:\n",
      "\tLoss:\t0.08702\n",
      "\tMAE:\t2828.86751\n",
      "\n",
      "\n",
      "Epoch 26\n",
      "\tTraining:\n",
      "\tLoss:\t0.09189\n",
      "\tMAE:\t2915.89469\n",
      "\tValidation:\n",
      "\tLoss:\t0.08495\n",
      "\tMAE:\t2822.84985\n",
      "\n",
      "\n",
      "Epoch 27\n",
      "\tTraining:\n",
      "\tLoss:\t0.08455\n",
      "\tMAE:\t2798.18571\n",
      "\tValidation:\n",
      "\tLoss:\t0.08905\n",
      "\tMAE:\t2862.64670\n",
      "\n",
      "\n",
      "Epoch 28\n",
      "\tTraining:\n",
      "\tLoss:\t0.09134\n",
      "\tMAE:\t2908.69693\n",
      "\tValidation:\n",
      "\tLoss:\t0.08781\n",
      "\tMAE:\t2859.42987\n",
      "\n",
      "\n",
      "Epoch 29\n",
      "\tTraining:\n",
      "\tLoss:\t0.08703\n",
      "\tMAE:\t2864.19166\n",
      "\tValidation:\n",
      "\tLoss:\t0.10098\n",
      "\tMAE:\t3039.29289\n",
      "\n",
      "\n",
      "Epoch 30\n",
      "\tTraining:\n",
      "\tLoss:\t0.08624\n",
      "\tMAE:\t2840.18317\n",
      "\tValidation:\n",
      "\tLoss:\t0.08320\n",
      "\tMAE:\t2791.16355\n",
      "\n",
      "\n",
      "Epoch 31\n",
      "\tTraining:\n",
      "\tLoss:\t0.08480\n",
      "\tMAE:\t2832.40800\n",
      "\tValidation:\n",
      "\tLoss:\t0.08724\n",
      "\tMAE:\t2850.80287\n",
      "\n",
      "\n",
      "Epoch 32\n",
      "\tTraining:\n",
      "\tLoss:\t0.08140\n",
      "\tMAE:\t2747.79611\n",
      "\tValidation:\n",
      "\tLoss:\t0.08433\n",
      "\tMAE:\t2774.72135\n",
      "\n",
      "\n",
      "Epoch 33\n",
      "\tTraining:\n",
      "\tLoss:\t0.08143\n",
      "\tMAE:\t2760.31940\n",
      "\tValidation:\n",
      "\tLoss:\t0.08196\n",
      "\tMAE:\t2747.60726\n",
      "\n",
      "\n",
      "Epoch 34\n",
      "\tTraining:\n",
      "\tLoss:\t0.07950\n",
      "\tMAE:\t2721.96195\n",
      "\tValidation:\n",
      "\tLoss:\t0.07874\n",
      "\tMAE:\t2715.40054\n",
      "\n",
      "\n",
      "Epoch 35\n",
      "\tTraining:\n",
      "\tLoss:\t0.07705\n",
      "\tMAE:\t2657.69074\n",
      "\tValidation:\n",
      "\tLoss:\t0.07753\n",
      "\tMAE:\t2666.20503\n",
      "\n",
      "\n",
      "Epoch 36\n",
      "\tTraining:\n",
      "\tLoss:\t0.07967\n",
      "\tMAE:\t2779.36445\n",
      "\tValidation:\n",
      "\tLoss:\t0.08492\n",
      "\tMAE:\t2773.44119\n",
      "\n",
      "\n",
      "Epoch 37\n",
      "\tTraining:\n",
      "\tLoss:\t0.08417\n",
      "\tMAE:\t2772.06183\n",
      "\tValidation:\n",
      "\tLoss:\t0.08203\n",
      "\tMAE:\t2757.97087\n",
      "\n",
      "\n",
      "Epoch 38\n",
      "\tTraining:\n",
      "\tLoss:\t0.07981\n",
      "\tMAE:\t2679.38818\n",
      "\tValidation:\n",
      "\tLoss:\t0.08173\n",
      "\tMAE:\t2736.56815\n",
      "\n",
      "\n",
      "Epoch 39\n",
      "\tTraining:\n",
      "\tLoss:\t0.08511\n",
      "\tMAE:\t2770.20534\n",
      "\tValidation:\n",
      "\tLoss:\t0.07790\n",
      "\tMAE:\t2691.75862\n",
      "\n",
      "\n",
      "Epoch 40\n",
      "\tTraining:\n",
      "\tLoss:\t0.07571\n",
      "\tMAE:\t2634.16883\n",
      "\tValidation:\n",
      "\tLoss:\t0.07841\n",
      "\tMAE:\t2677.03176\n",
      "\n",
      "\n",
      "Epoch 41\n",
      "\tTraining:\n",
      "\tLoss:\t0.07849\n",
      "\tMAE:\t2734.49499\n",
      "\tValidation:\n",
      "\tLoss:\t0.07894\n",
      "\tMAE:\t2708.56754\n",
      "\n",
      "\n",
      "Epoch 42\n",
      "\tTraining:\n",
      "\tLoss:\t0.07735\n",
      "\tMAE:\t2649.68641\n",
      "\tValidation:\n",
      "\tLoss:\t0.07689\n",
      "\tMAE:\t2689.71503\n",
      "\n",
      "\n",
      "Epoch 43\n",
      "\tTraining:\n",
      "\tLoss:\t0.07621\n",
      "\tMAE:\t2628.88061\n",
      "\tValidation:\n",
      "\tLoss:\t0.07510\n",
      "\tMAE:\t2627.34258\n",
      "\n",
      "\n",
      "Epoch 44\n",
      "\tTraining:\n",
      "\tLoss:\t0.07674\n",
      "\tMAE:\t2627.62649\n",
      "\tValidation:\n",
      "\tLoss:\t0.07423\n",
      "\tMAE:\t2622.88436\n",
      "\n",
      "\n",
      "Epoch 45\n",
      "\tTraining:\n",
      "\tLoss:\t0.07013\n",
      "\tMAE:\t2600.17959\n",
      "\tValidation:\n",
      "\tLoss:\t0.07705\n",
      "\tMAE:\t2677.80001\n",
      "\n",
      "\n",
      "Epoch 46\n",
      "\tTraining:\n",
      "\tLoss:\t0.07761\n",
      "\tMAE:\t2677.59950\n",
      "\tValidation:\n",
      "\tLoss:\t0.07371\n",
      "\tMAE:\t2601.02822\n",
      "\n",
      "\n",
      "Epoch 47\n",
      "\tTraining:\n",
      "\tLoss:\t0.07312\n",
      "\tMAE:\t2549.09734\n",
      "\tValidation:\n",
      "\tLoss:\t0.07674\n",
      "\tMAE:\t2680.06200\n",
      "\n",
      "\n",
      "Epoch 48\n",
      "\tTraining:\n",
      "\tLoss:\t0.07256\n",
      "\tMAE:\t2590.45214\n",
      "\tValidation:\n",
      "\tLoss:\t0.07734\n",
      "\tMAE:\t2664.05340\n",
      "\n",
      "\n",
      "Epoch 49\n",
      "\tTraining:\n",
      "\tLoss:\t0.07112\n",
      "\tMAE:\t2514.84027\n",
      "\tValidation:\n",
      "\tLoss:\t0.07420\n",
      "\tMAE:\t2623.81714\n",
      "\n",
      "\n",
      "Epoch 50\n",
      "\tTraining:\n",
      "\tLoss:\t0.07409\n",
      "\tMAE:\t2590.81149\n",
      "\tValidation:\n",
      "\tLoss:\t0.07414\n",
      "\tMAE:\t2639.24572\n",
      "\n",
      "\n",
      "Epoch 51\n",
      "\tTraining:\n",
      "\tLoss:\t0.07088\n",
      "\tMAE:\t2582.81270\n",
      "\tValidation:\n",
      "\tLoss:\t0.08167\n",
      "\tMAE:\t2731.18714\n",
      "\n",
      "\n",
      "Epoch 52\n",
      "\tTraining:\n",
      "\tLoss:\t0.07549\n",
      "\tMAE:\t2586.93329\n",
      "\tValidation:\n",
      "\tLoss:\t0.07214\n",
      "\tMAE:\t2589.47765\n",
      "\n",
      "\n",
      "Epoch 53\n",
      "\tTraining:\n",
      "\tLoss:\t0.07220\n",
      "\tMAE:\t2634.16020\n",
      "\tValidation:\n",
      "\tLoss:\t0.07727\n",
      "\tMAE:\t2692.91824\n",
      "\n",
      "\n",
      "Epoch 54\n",
      "\tTraining:\n",
      "\tLoss:\t0.07269\n",
      "\tMAE:\t2561.70966\n",
      "\tValidation:\n",
      "\tLoss:\t0.07622\n",
      "\tMAE:\t2652.65593\n",
      "\n",
      "\n",
      "Epoch 55\n",
      "\tTraining:\n",
      "\tLoss:\t0.06879\n",
      "\tMAE:\t2526.53292\n",
      "\tValidation:\n",
      "\tLoss:\t0.07426\n",
      "\tMAE:\t2614.80674\n",
      "\n",
      "\n",
      "Epoch 56\n",
      "\tTraining:\n",
      "\tLoss:\t0.07075\n",
      "\tMAE:\t2517.13159\n",
      "\tValidation:\n",
      "\tLoss:\t0.07402\n",
      "\tMAE:\t2623.96736\n",
      "\n",
      "\n",
      "Epoch 57\n",
      "\tTraining:\n",
      "\tLoss:\t0.06801\n",
      "\tMAE:\t2502.58446\n",
      "\tValidation:\n",
      "\tLoss:\t0.07341\n",
      "\tMAE:\t2590.35277\n",
      "\n",
      "\n",
      "Epoch 58\n",
      "\tTraining:\n",
      "\tLoss:\t0.06732\n",
      "\tMAE:\t2515.67461\n",
      "\tValidation:\n",
      "\tLoss:\t0.06971\n",
      "\tMAE:\t2539.65215\n",
      "\n",
      "\n",
      "Epoch 59\n",
      "\tTraining:\n",
      "\tLoss:\t0.06941\n",
      "\tMAE:\t2529.00534\n",
      "\tValidation:\n",
      "\tLoss:\t0.06956\n",
      "\tMAE:\t2544.00665\n",
      "\n",
      "\n",
      "Epoch 60\n",
      "\tTraining:\n",
      "\tLoss:\t0.06830\n",
      "\tMAE:\t2505.94645\n",
      "\tValidation:\n",
      "\tLoss:\t0.07002\n",
      "\tMAE:\t2567.57675\n",
      "\n",
      "\n",
      "Epoch 61\n",
      "\tTraining:\n",
      "\tLoss:\t0.06602\n",
      "\tMAE:\t2473.92318\n",
      "\tValidation:\n",
      "\tLoss:\t0.07002\n",
      "\tMAE:\t2557.13118\n",
      "\n",
      "\n",
      "Epoch 62\n",
      "\tTraining:\n",
      "\tLoss:\t0.07212\n",
      "\tMAE:\t2573.64905\n",
      "\tValidation:\n",
      "\tLoss:\t0.06562\n",
      "\tMAE:\t2461.03717\n",
      "\n",
      "\n",
      "Epoch 63\n",
      "\tTraining:\n",
      "\tLoss:\t0.06781\n",
      "\tMAE:\t2478.38707\n",
      "\tValidation:\n",
      "\tLoss:\t0.06937\n",
      "\tMAE:\t2571.88541\n",
      "\n",
      "\n",
      "Epoch 64\n",
      "\tTraining:\n",
      "\tLoss:\t0.07006\n",
      "\tMAE:\t2477.82730\n",
      "\tValidation:\n",
      "\tLoss:\t0.06969\n",
      "\tMAE:\t2570.51900\n",
      "\n",
      "\n",
      "Epoch 65\n",
      "\tTraining:\n",
      "\tLoss:\t0.06842\n",
      "\tMAE:\t2538.97318\n",
      "\tValidation:\n",
      "\tLoss:\t0.07334\n",
      "\tMAE:\t2611.68362\n",
      "\n",
      "\n",
      "Epoch 66\n",
      "\tTraining:\n",
      "\tLoss:\t0.06730\n",
      "\tMAE:\t2455.00757\n",
      "\tValidation:\n",
      "\tLoss:\t0.07070\n",
      "\tMAE:\t2579.08796\n",
      "\n",
      "\n",
      "Epoch 67\n",
      "\tTraining:\n",
      "\tLoss:\t0.06567\n",
      "\tMAE:\t2499.14096\n",
      "\tValidation:\n",
      "\tLoss:\t0.07006\n",
      "\tMAE:\t2569.30369\n",
      "\n",
      "\n",
      "Epoch 68\n",
      "\tTraining:\n",
      "\tLoss:\t0.06518\n",
      "\tMAE:\t2410.29008\n",
      "\tValidation:\n",
      "\tLoss:\t0.06957\n",
      "\tMAE:\t2545.05130\n",
      "\n",
      "\n",
      "Epoch 69\n",
      "\tTraining:\n",
      "\tLoss:\t0.06438\n",
      "\tMAE:\t2413.33445\n",
      "\tValidation:\n",
      "\tLoss:\t0.06391\n",
      "\tMAE:\t2441.87913\n",
      "\n",
      "\n",
      "Epoch 70\n",
      "\tTraining:\n",
      "\tLoss:\t0.06469\n",
      "\tMAE:\t2447.93465\n",
      "\tValidation:\n",
      "\tLoss:\t0.06693\n",
      "\tMAE:\t2482.36644\n",
      "\n",
      "\n",
      "Epoch 71\n",
      "\tTraining:\n",
      "\tLoss:\t0.06455\n",
      "\tMAE:\t2409.96642\n",
      "\tValidation:\n",
      "\tLoss:\t0.06798\n",
      "\tMAE:\t2509.99242\n",
      "\n",
      "\n",
      "Epoch 72\n",
      "\tTraining:\n",
      "\tLoss:\t0.06767\n",
      "\tMAE:\t2468.54484\n",
      "\tValidation:\n",
      "\tLoss:\t0.07163\n",
      "\tMAE:\t2570.45648\n",
      "\n",
      "\n",
      "Epoch 73\n",
      "\tTraining:\n",
      "\tLoss:\t0.06571\n",
      "\tMAE:\t2475.11713\n",
      "\tValidation:\n",
      "\tLoss:\t0.06687\n",
      "\tMAE:\t2492.21843\n",
      "\n",
      "\n",
      "Epoch 74\n",
      "\tTraining:\n",
      "\tLoss:\t0.06471\n",
      "\tMAE:\t2503.17772\n",
      "\tValidation:\n",
      "\tLoss:\t0.06480\n",
      "\tMAE:\t2458.38555\n",
      "\n",
      "\n",
      "Epoch 75\n",
      "\tTraining:\n",
      "\tLoss:\t0.06086\n",
      "\tMAE:\t2426.57269\n",
      "\tValidation:\n",
      "\tLoss:\t0.07099\n",
      "\tMAE:\t2571.17750\n",
      "\n",
      "\n",
      "Epoch 76\n",
      "\tTraining:\n",
      "\tLoss:\t0.06029\n",
      "\tMAE:\t2328.93463\n",
      "\tValidation:\n",
      "\tLoss:\t0.06294\n",
      "\tMAE:\t2423.41182\n",
      "\n",
      "\n",
      "Epoch 77\n",
      "\tTraining:\n",
      "\tLoss:\t0.06286\n",
      "\tMAE:\t2379.77110\n",
      "\tValidation:\n",
      "\tLoss:\t0.06366\n",
      "\tMAE:\t2435.08291\n",
      "\n",
      "\n",
      "Epoch 78\n",
      "\tTraining:\n",
      "\tLoss:\t0.06440\n",
      "\tMAE:\t2425.68464\n",
      "\tValidation:\n",
      "\tLoss:\t0.06917\n",
      "\tMAE:\t2544.22447\n",
      "\n",
      "\n",
      "Epoch 79\n",
      "\tTraining:\n",
      "\tLoss:\t0.06330\n",
      "\tMAE:\t2422.23479\n",
      "\tValidation:\n",
      "\tLoss:\t0.06350\n",
      "\tMAE:\t2448.89756\n",
      "\n",
      "\n",
      "Epoch 80\n",
      "\tTraining:\n",
      "\tLoss:\t0.06008\n",
      "\tMAE:\t2353.65910\n",
      "\tValidation:\n",
      "\tLoss:\t0.06827\n",
      "\tMAE:\t2551.81027\n",
      "\n",
      "\n",
      "Epoch 81\n",
      "\tTraining:\n",
      "\tLoss:\t0.05994\n",
      "\tMAE:\t2370.83080\n",
      "\tValidation:\n",
      "\tLoss:\t0.06211\n",
      "\tMAE:\t2389.10342\n",
      "\n",
      "\n",
      "Epoch 82\n",
      "\tTraining:\n",
      "\tLoss:\t0.06138\n",
      "\tMAE:\t2369.13777\n",
      "\tValidation:\n",
      "\tLoss:\t0.07212\n",
      "\tMAE:\t2620.59716\n",
      "\n",
      "\n",
      "Epoch 83\n",
      "\tTraining:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t0.05641\n",
      "\tMAE:\t2249.93825\n",
      "\tValidation:\n",
      "\tLoss:\t0.06296\n",
      "\tMAE:\t2409.10982\n",
      "\n",
      "\n",
      "Epoch 84\n",
      "\tTraining:\n",
      "\tLoss:\t0.05771\n",
      "\tMAE:\t2338.62519\n",
      "\tValidation:\n",
      "\tLoss:\t0.06331\n",
      "\tMAE:\t2427.66649\n",
      "\n",
      "\n",
      "Epoch 85\n",
      "\tTraining:\n",
      "\tLoss:\t0.06523\n",
      "\tMAE:\t2426.93880\n",
      "\tValidation:\n",
      "\tLoss:\t0.06267\n",
      "\tMAE:\t2423.04785\n",
      "\n",
      "\n",
      "Epoch 86\n",
      "\tTraining:\n",
      "\tLoss:\t0.05989\n",
      "\tMAE:\t2354.11109\n",
      "\tValidation:\n",
      "\tLoss:\t0.06236\n",
      "\tMAE:\t2391.22880\n",
      "\n",
      "\n",
      "Epoch 87\n",
      "\tTraining:\n",
      "\tLoss:\t0.05847\n",
      "\tMAE:\t2303.26594\n",
      "\tValidation:\n",
      "\tLoss:\t0.06383\n",
      "\tMAE:\t2417.18688\n",
      "\n",
      "\n",
      "Epoch 88\n",
      "\tTraining:\n",
      "\tLoss:\t0.06165\n",
      "\tMAE:\t2383.59711\n",
      "\tValidation:\n",
      "\tLoss:\t0.06338\n",
      "\tMAE:\t2438.16338\n",
      "\n",
      "\n",
      "Epoch 89\n",
      "\tTraining:\n",
      "\tLoss:\t0.05437\n",
      "\tMAE:\t2220.58772\n",
      "\tValidation:\n",
      "\tLoss:\t0.06123\n",
      "\tMAE:\t2388.94110\n",
      "\n",
      "\n",
      "Epoch 90\n",
      "\tTraining:\n",
      "\tLoss:\t0.05711\n",
      "\tMAE:\t2325.01708\n",
      "\tValidation:\n",
      "\tLoss:\t0.06235\n",
      "\tMAE:\t2397.16827\n",
      "\n",
      "\n",
      "Epoch 91\n",
      "\tTraining:\n",
      "\tLoss:\t0.05729\n",
      "\tMAE:\t2248.00828\n",
      "\tValidation:\n",
      "\tLoss:\t0.06005\n",
      "\tMAE:\t2348.97831\n",
      "\n",
      "\n",
      "Epoch 92\n",
      "\tTraining:\n",
      "\tLoss:\t0.05606\n",
      "\tMAE:\t2265.25553\n",
      "\tValidation:\n",
      "\tLoss:\t0.06194\n",
      "\tMAE:\t2376.35530\n",
      "\n",
      "\n",
      "Epoch 93\n",
      "\tTraining:\n",
      "\tLoss:\t0.05933\n",
      "\tMAE:\t2343.13376\n",
      "\tValidation:\n",
      "\tLoss:\t0.06114\n",
      "\tMAE:\t2382.10688\n",
      "\n",
      "\n",
      "Epoch 94\n",
      "\tTraining:\n",
      "\tLoss:\t0.06231\n",
      "\tMAE:\t2376.83773\n",
      "\tValidation:\n",
      "\tLoss:\t0.06069\n",
      "\tMAE:\t2372.47611\n",
      "\n",
      "\n",
      "Epoch 95\n",
      "\tTraining:\n",
      "\tLoss:\t0.05886\n",
      "\tMAE:\t2310.76198\n",
      "\tValidation:\n",
      "\tLoss:\t0.06055\n",
      "\tMAE:\t2367.23643\n",
      "\n",
      "\n",
      "Epoch 96\n",
      "\tTraining:\n",
      "\tLoss:\t0.05570\n",
      "\tMAE:\t2287.99177\n",
      "\tValidation:\n",
      "\tLoss:\t0.06487\n",
      "\tMAE:\t2463.10032\n",
      "\n",
      "\n",
      "Epoch 97\n",
      "\tTraining:\n",
      "\tLoss:\t0.05651\n",
      "\tMAE:\t2271.16034\n",
      "\tValidation:\n",
      "\tLoss:\t0.06732\n",
      "\tMAE:\t2493.81880\n",
      "\n",
      "\n",
      "Epoch 98\n",
      "\tTraining:\n",
      "\tLoss:\t0.05783\n",
      "\tMAE:\t2303.77497\n",
      "\tValidation:\n",
      "\tLoss:\t0.05941\n",
      "\tMAE:\t2342.33793\n",
      "\n",
      "\n",
      "Epoch 99\n",
      "\tTraining:\n",
      "\tLoss:\t0.05543\n",
      "\tMAE:\t2260.22398\n",
      "\tValidation:\n",
      "\tLoss:\t0.07374\n",
      "\tMAE:\t2629.51340\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist_train, hist_val = Train(model,100,opt,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot((hist_train),label='train_loss')\n",
    "plt.plot((hist_val),label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"models/conv_1d.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"models/conv_1d.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval:\n",
      "\tLoss:\t0.07527\n",
      "\tMAE:\t2616.20195\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try sequential conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), sizes=[64,96]):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(n_tokens, 64, padding_idx=PAD_IX)\n",
    "        self.conv1 = nn.Conv1d(64, sizes[0], kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(sizes[0], sizes[1], kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm1d(sizes[-1])\n",
    "        self.pool1 = GlobalMaxPooling()        \n",
    "        self.dense = nn.Linear(sizes[-1], sizes[-1])\n",
    "\n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, size1+size2]\n",
    "        \"\"\"\n",
    "        h = self.emb(text_ix)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        h = torch.transpose(h, 1, 2)\n",
    "        \n",
    "        # Apply the layers as defined above. Add some ReLUs before dense.\n",
    "        h = self.conv1(h)\n",
    "        h = self.conv2(h)\n",
    "        h = self.pool1(h)\n",
    "        h = self.bn(h)\n",
    "        h = F.relu(self.dense(h)) \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an encoder for job descriptions.\n",
    "# Use any means you want so long as it's torch.nn.Module.\n",
    "class DescriptionEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), sizes=[48,48,48]):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(n_tokens, 64, padding_idx=PAD_IX)\n",
    "        self.conv1 = nn.Conv1d(64, sizes[0], kernel_size=5, padding=1)\n",
    "        self.conv2 = nn.Conv1d(sizes[0], sizes[1], kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(sizes[1], sizes[2], kernel_size=3,padding=1)\n",
    "        self.pool1 = GlobalMaxPooling()    \n",
    "        self.bn = nn.BatchNorm1d(sizes[-1])\n",
    "        self.dense = nn.Linear(sizes[-1], sizes[-1])\n",
    "        \n",
    "\n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        h = self.emb(text_ix)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        h = torch.transpose(h, 1, 2)\n",
    "        \n",
    "        # Apply the layers as defined above. Add some ReLUs before dense.\n",
    "        h = self.conv1(h)\n",
    "        h = self.conv2(h)\n",
    "        h = self.conv3(h)\n",
    "        h = self.pool1(h)\n",
    "        h = self.bn(h)\n",
    "        h = F.relu(self.dense(h))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This class does all the steps from (title, desc, categorical) features -> predicted target\n",
    "    It unites title & desc encoders you defined above as long as some layers for head and categorical branch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_),\n",
    "                t_sizes = [64,96], d_sizes = [64,80,100]):\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.title_encoder = TitleEncoder(sizes=t_sizes)\n",
    "        self.desc_encoder = DescriptionEncoder(sizes=d_sizes)\n",
    "        \n",
    "        # define layers for categorical features. A few dense layers would do.\n",
    "        l_dims = [300,100,20]\n",
    "        self.cat_encoder = CategoricalEncoder(n_cat_features,l_dims)\n",
    "        \n",
    "        # define \"output\" layers that process depend the three encoded vectors into answer\n",
    "        n_concat = t_sizes[-1] + d_sizes[-1] + l_dims[-1]\n",
    "        self.out_layers = OutputLayers(n_concat,[100,80,30])\n",
    "        \n",
    "        \n",
    "    def forward(self, title_ix, desc_ix, cat_features):\n",
    "        \"\"\"\n",
    "        :param title_ix: int32 Variable [batch, title_len], job titles encoded by as_matrix\n",
    "        :param desc_ix:  int32 Variable [batch, desc_len] , job descriptions encoded by as_matrix\n",
    "        :param cat_features: float32 Variable [batch, n_cat_features]\n",
    "        :returns: float32 Variable 1d [batch], predicted log1p-salary\n",
    "        \"\"\"\n",
    "        \n",
    "        # process each data source with it's respective encoder\n",
    "        title_h = self.title_encoder(title_ix)\n",
    "        desc_h = self.desc_encoder(desc_ix)\n",
    "        \n",
    "        # apply categorical encoder\n",
    "        cat_h = self.cat_encoder(cat_features)\n",
    "        \n",
    "        # concatenate all vectors together...\n",
    "        joint_h = torch.cat([title_h, desc_h, cat_h], dim=1)\n",
    "        \n",
    "        # ... and stack a few more layers at the top\n",
    "        output = self.out_layers(joint_h)\n",
    "        \n",
    "        # Note 1: do not forget to select first columns, [:, 0], to get to 1d outputs\n",
    "        # Note 2: please do not use output nonlinearities.\n",
    "        \n",
    "        return output[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullNetwork(\n",
       "  (title_encoder): TitleEncoder(\n",
       "    (emb): Embedding(32456, 64, padding_idx=1)\n",
       "    (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(64, 96, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pool1): GlobalMaxPooling()\n",
       "    (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "  )\n",
       "  (desc_encoder): DescriptionEncoder(\n",
       "    (emb): Embedding(32456, 64, padding_idx=1)\n",
       "    (conv1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv3): Conv1d(64, 96, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (pool1): GlobalMaxPooling()\n",
       "    (bn): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "  )\n",
       "  (cat_encoder): CategoricalEncoder(\n",
       "    (dense): ModuleList(\n",
       "      (0): Linear(in_features=3768, out_features=300, bias=True)\n",
       "      (1): Linear(in_features=300, out_features=100, bias=True)\n",
       "      (2): Linear(in_features=100, out_features=20, bias=True)\n",
       "    )\n",
       "    (bn): ModuleList(\n",
       "      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (out_layers): OutputLayers(\n",
       "    (dense): ModuleList(\n",
       "      (0): Linear(in_features=212, out_features=100, bias=True)\n",
       "      (1): Linear(in_features=100, out_features=80, bias=True)\n",
       "      (2): Linear(in_features=80, out_features=30, bias=True)\n",
       "    )\n",
       "    (bn): ModuleList(\n",
       "      (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (last_layer): Linear(in_features=30, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FullNetwork(t_sizes=[64,96],d_sizes=[64,64,96])\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb21a8dda86c44d89b403d06d291338e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tTraining:\n",
      "\tLoss:\t93.15306\n",
      "\tMAE:\t12507.16582\n",
      "\tValidation:\n",
      "\tLoss:\t79.09775\n",
      "\tMAE:\t12605.12507\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "\tTraining:\n",
      "\tLoss:\t63.70099\n",
      "\tMAE:\t12520.28475\n",
      "\tValidation:\n",
      "\tLoss:\t49.28607\n",
      "\tMAE:\t12596.34620\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "\tTraining:\n",
      "\tLoss:\t33.47060\n",
      "\tMAE:\t12525.67311\n",
      "\tValidation:\n",
      "\tLoss:\t20.64282\n",
      "\tMAE:\t12468.30087\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "\tTraining:\n",
      "\tLoss:\t11.64469\n",
      "\tMAE:\t12022.00437\n",
      "\tValidation:\n",
      "\tLoss:\t5.83191\n",
      "\tMAE:\t11490.94605\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "\tTraining:\n",
      "\tLoss:\t2.40218\n",
      "\tMAE:\t9442.73082\n",
      "\tValidation:\n",
      "\tLoss:\t0.83184\n",
      "\tMAE:\t7234.81715\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "\tTraining:\n",
      "\tLoss:\t0.41077\n",
      "\tMAE:\t5447.28815\n",
      "\tValidation:\n",
      "\tLoss:\t0.22429\n",
      "\tMAE:\t4408.87462\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "\tTraining:\n",
      "\tLoss:\t0.18293\n",
      "\tMAE:\t4015.95371\n",
      "\tValidation:\n",
      "\tLoss:\t0.14282\n",
      "\tMAE:\t3643.64660\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "\tTraining:\n",
      "\tLoss:\t0.14424\n",
      "\tMAE:\t3661.37681\n",
      "\tValidation:\n",
      "\tLoss:\t0.14411\n",
      "\tMAE:\t3702.13402\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "\tTraining:\n",
      "\tLoss:\t0.14780\n",
      "\tMAE:\t3658.03775\n",
      "\tValidation:\n",
      "\tLoss:\t0.13637\n",
      "\tMAE:\t3547.73705\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "\tTraining:\n",
      "\tLoss:\t0.13102\n",
      "\tMAE:\t3444.18886\n",
      "\tValidation:\n",
      "\tLoss:\t0.13435\n",
      "\tMAE:\t3521.62380\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "\tTraining:\n",
      "\tLoss:\t0.13263\n",
      "\tMAE:\t3528.53218\n",
      "\tValidation:\n",
      "\tLoss:\t0.13129\n",
      "\tMAE:\t3456.77497\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "\tTraining:\n",
      "\tLoss:\t0.13118\n",
      "\tMAE:\t3543.21692\n",
      "\tValidation:\n",
      "\tLoss:\t0.12589\n",
      "\tMAE:\t3389.98720\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "\tTraining:\n",
      "\tLoss:\t0.11821\n",
      "\tMAE:\t3321.67002\n",
      "\tValidation:\n",
      "\tLoss:\t0.11005\n",
      "\tMAE:\t3186.45563\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "\tTraining:\n",
      "\tLoss:\t0.12088\n",
      "\tMAE:\t3311.09197\n",
      "\tValidation:\n",
      "\tLoss:\t0.11380\n",
      "\tMAE:\t3240.54329\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "\tTraining:\n",
      "\tLoss:\t0.11790\n",
      "\tMAE:\t3292.67321\n",
      "\tValidation:\n",
      "\tLoss:\t0.11031\n",
      "\tMAE:\t3194.43257\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "\tTraining:\n",
      "\tLoss:\t0.11095\n",
      "\tMAE:\t3239.05829\n",
      "\tValidation:\n",
      "\tLoss:\t0.10366\n",
      "\tMAE:\t3088.31582\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "\tTraining:\n",
      "\tLoss:\t0.11551\n",
      "\tMAE:\t3266.26365\n",
      "\tValidation:\n",
      "\tLoss:\t0.10145\n",
      "\tMAE:\t3065.69618\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "\tTraining:\n",
      "\tLoss:\t0.11476\n",
      "\tMAE:\t3246.28643\n",
      "\tValidation:\n",
      "\tLoss:\t0.11843\n",
      "\tMAE:\t3256.19547\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "\tTraining:\n",
      "\tLoss:\t0.10806\n",
      "\tMAE:\t3140.17378\n",
      "\tValidation:\n",
      "\tLoss:\t0.11009\n",
      "\tMAE:\t3189.39625\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "\tTraining:\n",
      "\tLoss:\t0.10452\n",
      "\tMAE:\t3079.20646\n",
      "\tValidation:\n",
      "\tLoss:\t0.10119\n",
      "\tMAE:\t3061.22571\n",
      "\n",
      "\n",
      "Epoch 20\n",
      "\tTraining:\n",
      "\tLoss:\t0.10830\n",
      "\tMAE:\t3173.51874\n",
      "\tValidation:\n",
      "\tLoss:\t0.09825\n",
      "\tMAE:\t3011.94035\n",
      "\n",
      "\n",
      "Epoch 21\n",
      "\tTraining:\n",
      "\tLoss:\t0.10638\n",
      "\tMAE:\t3102.10708\n",
      "\tValidation:\n",
      "\tLoss:\t0.11019\n",
      "\tMAE:\t3180.44505\n",
      "\n",
      "\n",
      "Epoch 22\n",
      "\tTraining:\n",
      "\tLoss:\t0.10696\n",
      "\tMAE:\t3134.28511\n",
      "\tValidation:\n",
      "\tLoss:\t0.09478\n",
      "\tMAE:\t2967.83758\n",
      "\n",
      "\n",
      "Epoch 23\n",
      "\tTraining:\n",
      "\tLoss:\t0.10513\n",
      "\tMAE:\t3129.60776\n",
      "\tValidation:\n",
      "\tLoss:\t0.11492\n",
      "\tMAE:\t3223.62319\n",
      "\n",
      "\n",
      "Epoch 24\n",
      "\tTraining:\n",
      "\tLoss:\t0.10196\n",
      "\tMAE:\t3068.28573\n",
      "\tValidation:\n",
      "\tLoss:\t0.10525\n",
      "\tMAE:\t3100.97534\n",
      "\n",
      "\n",
      "Epoch 25\n",
      "\tTraining:\n",
      "\tLoss:\t0.09734\n",
      "\tMAE:\t2969.21664\n",
      "\tValidation:\n",
      "\tLoss:\t0.10683\n",
      "\tMAE:\t3155.31983\n",
      "\n",
      "\n",
      "Epoch 26\n",
      "\tTraining:\n",
      "\tLoss:\t0.10015\n",
      "\tMAE:\t3055.90410\n",
      "\tValidation:\n",
      "\tLoss:\t0.09766\n",
      "\tMAE:\t3009.94644\n",
      "\n",
      "\n",
      "Epoch 27\n",
      "\tTraining:\n",
      "\tLoss:\t0.09449\n",
      "\tMAE:\t2930.58480\n",
      "\tValidation:\n",
      "\tLoss:\t0.10371\n",
      "\tMAE:\t3075.13864\n",
      "\n",
      "\n",
      "Epoch 28\n",
      "\tTraining:\n",
      "\tLoss:\t0.09174\n",
      "\tMAE:\t2880.16659\n",
      "\tValidation:\n",
      "\tLoss:\t0.10265\n",
      "\tMAE:\t3131.14856\n",
      "\n",
      "\n",
      "Epoch 29\n",
      "\tTraining:\n",
      "\tLoss:\t0.10334\n",
      "\tMAE:\t3054.94776\n",
      "\tValidation:\n",
      "\tLoss:\t0.10013\n",
      "\tMAE:\t3010.09901\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist_train, hist_val = Train(model,30,opt,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98734735c9b440d2b1d29a454246af74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tTraining:\n",
      "\tLoss:\t0.08862\n",
      "\tMAE:\t2892.74067\n",
      "\tValidation:\n",
      "\tLoss:\t0.08783\n",
      "\tMAE:\t2834.08291\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "\tTraining:\n",
      "\tLoss:\t0.09264\n",
      "\tMAE:\t2903.86362\n",
      "\tValidation:\n",
      "\tLoss:\t0.10977\n",
      "\tMAE:\t3170.38996\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "\tTraining:\n",
      "\tLoss:\t0.10216\n",
      "\tMAE:\t3130.34315\n",
      "\tValidation:\n",
      "\tLoss:\t0.09761\n",
      "\tMAE:\t2989.75183\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "\tTraining:\n",
      "\tLoss:\t0.09147\n",
      "\tMAE:\t2948.83047\n",
      "\tValidation:\n",
      "\tLoss:\t0.08809\n",
      "\tMAE:\t2834.73781\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "\tTraining:\n",
      "\tLoss:\t0.08990\n",
      "\tMAE:\t2871.09671\n",
      "\tValidation:\n",
      "\tLoss:\t0.09080\n",
      "\tMAE:\t2871.28841\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "\tTraining:\n",
      "\tLoss:\t0.09363\n",
      "\tMAE:\t2950.14489\n",
      "\tValidation:\n",
      "\tLoss:\t0.08522\n",
      "\tMAE:\t2828.97734\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "\tTraining:\n",
      "\tLoss:\t0.09419\n",
      "\tMAE:\t2947.78767\n",
      "\tValidation:\n",
      "\tLoss:\t0.10121\n",
      "\tMAE:\t2994.89893\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "\tTraining:\n",
      "\tLoss:\t0.08651\n",
      "\tMAE:\t2809.40644\n",
      "\tValidation:\n",
      "\tLoss:\t0.13287\n",
      "\tMAE:\t3435.18062\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "\tTraining:\n",
      "\tLoss:\t0.08614\n",
      "\tMAE:\t2786.51264\n",
      "\tValidation:\n",
      "\tLoss:\t0.08811\n",
      "\tMAE:\t2867.62917\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "\tTraining:\n",
      "\tLoss:\t0.08406\n",
      "\tMAE:\t2764.61719\n",
      "\tValidation:\n",
      "\tLoss:\t0.10235\n",
      "\tMAE:\t2979.99289\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "\tTraining:\n",
      "\tLoss:\t0.08894\n",
      "\tMAE:\t2814.74666\n",
      "\tValidation:\n",
      "\tLoss:\t0.08476\n",
      "\tMAE:\t2771.89363\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "\tTraining:\n",
      "\tLoss:\t0.08359\n",
      "\tMAE:\t2769.75444\n",
      "\tValidation:\n",
      "\tLoss:\t0.08308\n",
      "\tMAE:\t2784.23814\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "\tTraining:\n",
      "\tLoss:\t0.08693\n",
      "\tMAE:\t2805.74719\n",
      "\tValidation:\n",
      "\tLoss:\t0.08197\n",
      "\tMAE:\t2735.09626\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "\tTraining:\n",
      "\tLoss:\t0.08680\n",
      "\tMAE:\t2827.72927\n",
      "\tValidation:\n",
      "\tLoss:\t0.08164\n",
      "\tMAE:\t2745.74778\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "\tTraining:\n",
      "\tLoss:\t0.08172\n",
      "\tMAE:\t2744.32998\n",
      "\tValidation:\n",
      "\tLoss:\t0.08311\n",
      "\tMAE:\t2782.85537\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "\tTraining:\n",
      "\tLoss:\t0.08124\n",
      "\tMAE:\t2793.08568\n",
      "\tValidation:\n",
      "\tLoss:\t0.07914\n",
      "\tMAE:\t2709.83152\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "\tTraining:\n",
      "\tLoss:\t0.07967\n",
      "\tMAE:\t2720.01935\n",
      "\tValidation:\n",
      "\tLoss:\t0.07874\n",
      "\tMAE:\t2691.90041\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "\tTraining:\n",
      "\tLoss:\t0.08610\n",
      "\tMAE:\t2824.09699\n",
      "\tValidation:\n",
      "\tLoss:\t0.07642\n",
      "\tMAE:\t2634.27938\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "\tTraining:\n",
      "\tLoss:\t0.08233\n",
      "\tMAE:\t2745.65820\n",
      "\tValidation:\n",
      "\tLoss:\t0.07766\n",
      "\tMAE:\t2654.62416\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "\tTraining:\n",
      "\tLoss:\t0.08603\n",
      "\tMAE:\t2832.04821\n",
      "\tValidation:\n",
      "\tLoss:\t0.09632\n",
      "\tMAE:\t3016.62747\n",
      "\n",
      "\n",
      "Epoch 20\n",
      "\tTraining:\n",
      "\tLoss:\t0.08102\n",
      "\tMAE:\t2754.78157\n",
      "\tValidation:\n",
      "\tLoss:\t0.08103\n",
      "\tMAE:\t2723.92833\n",
      "\n",
      "\n",
      "Epoch 21\n",
      "\tTraining:\n",
      "\tLoss:\t0.08079\n",
      "\tMAE:\t2720.87223\n",
      "\tValidation:\n",
      "\tLoss:\t0.08701\n",
      "\tMAE:\t2829.02378\n",
      "\n",
      "\n",
      "Epoch 22\n",
      "\tTraining:\n",
      "\tLoss:\t0.07923\n",
      "\tMAE:\t2720.02948\n",
      "\tValidation:\n",
      "\tLoss:\t0.07997\n",
      "\tMAE:\t2717.88763\n",
      "\n",
      "\n",
      "Epoch 23\n",
      "\tTraining:\n",
      "\tLoss:\t0.08131\n",
      "\tMAE:\t2749.61135\n",
      "\tValidation:\n",
      "\tLoss:\t0.07915\n",
      "\tMAE:\t2676.07603\n",
      "\n",
      "\n",
      "Epoch 24\n",
      "\tTraining:\n",
      "\tLoss:\t0.08310\n",
      "\tMAE:\t2787.08581\n",
      "\tValidation:\n",
      "\tLoss:\t0.07800\n",
      "\tMAE:\t2655.37141\n",
      "\n",
      "\n",
      "Epoch 25\n",
      "\tTraining:\n",
      "\tLoss:\t0.07648\n",
      "\tMAE:\t2699.89770\n",
      "\tValidation:\n",
      "\tLoss:\t0.07402\n",
      "\tMAE:\t2587.90797\n",
      "\n",
      "\n",
      "Epoch 26\n",
      "\tTraining:\n",
      "\tLoss:\t0.07845\n",
      "\tMAE:\t2695.26368\n",
      "\tValidation:\n",
      "\tLoss:\t0.08374\n",
      "\tMAE:\t2759.92782\n",
      "\n",
      "\n",
      "Epoch 27\n",
      "\tTraining:\n",
      "\tLoss:\t0.07484\n",
      "\tMAE:\t2622.21770\n",
      "\tValidation:\n",
      "\tLoss:\t0.07527\n",
      "\tMAE:\t2618.85574\n",
      "\n",
      "\n",
      "Epoch 28\n",
      "\tTraining:\n",
      "\tLoss:\t0.07816\n",
      "\tMAE:\t2646.99328\n",
      "\tValidation:\n",
      "\tLoss:\t0.07542\n",
      "\tMAE:\t2641.18144\n",
      "\n",
      "\n",
      "Epoch 29\n",
      "\tTraining:\n",
      "\tLoss:\t0.07322\n",
      "\tMAE:\t2605.30468\n",
      "\tValidation:\n",
      "\tLoss:\t0.08347\n",
      "\tMAE:\t2739.61665\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist_train, hist_val = Train(model,30,opt,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9762c36b264a28b6eca1edddd37937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tTraining:\n",
      "\tLoss:\t0.07391\n",
      "\tMAE:\t2588.22879\n",
      "\tValidation:\n",
      "\tLoss:\t0.07322\n",
      "\tMAE:\t2579.45926\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "\tTraining:\n",
      "\tLoss:\t0.07698\n",
      "\tMAE:\t2652.60796\n",
      "\tValidation:\n",
      "\tLoss:\t0.07255\n",
      "\tMAE:\t2569.07401\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "\tTraining:\n",
      "\tLoss:\t0.07860\n",
      "\tMAE:\t2672.36688\n",
      "\tValidation:\n",
      "\tLoss:\t0.07599\n",
      "\tMAE:\t2620.44395\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "\tTraining:\n",
      "\tLoss:\t0.07987\n",
      "\tMAE:\t2694.35638\n",
      "\tValidation:\n",
      "\tLoss:\t0.07086\n",
      "\tMAE:\t2532.94814\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "\tTraining:\n",
      "\tLoss:\t0.07380\n",
      "\tMAE:\t2588.77022\n",
      "\tValidation:\n",
      "\tLoss:\t0.07710\n",
      "\tMAE:\t2700.39670\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "\tTraining:\n",
      "\tLoss:\t0.07175\n",
      "\tMAE:\t2585.44228\n",
      "\tValidation:\n",
      "\tLoss:\t0.07363\n",
      "\tMAE:\t2596.64253\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "\tTraining:\n",
      "\tLoss:\t0.06993\n",
      "\tMAE:\t2529.26897\n",
      "\tValidation:\n",
      "\tLoss:\t0.07546\n",
      "\tMAE:\t2664.69852\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "\tTraining:\n",
      "\tLoss:\t0.07529\n",
      "\tMAE:\t2630.90601\n",
      "\tValidation:\n",
      "\tLoss:\t0.07382\n",
      "\tMAE:\t2597.36595\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "\tTraining:\n",
      "\tLoss:\t0.07205\n",
      "\tMAE:\t2554.27341\n",
      "\tValidation:\n",
      "\tLoss:\t0.08119\n",
      "\tMAE:\t2738.62479\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "\tTraining:\n",
      "\tLoss:\t0.07455\n",
      "\tMAE:\t2661.36950\n",
      "\tValidation:\n",
      "\tLoss:\t0.07525\n",
      "\tMAE:\t2647.53205\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "\tTraining:\n",
      "\tLoss:\t0.07067\n",
      "\tMAE:\t2599.71802\n",
      "\tValidation:\n",
      "\tLoss:\t0.08507\n",
      "\tMAE:\t2799.22137\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "\tTraining:\n",
      "\tLoss:\t0.07245\n",
      "\tMAE:\t2577.08809\n",
      "\tValidation:\n",
      "\tLoss:\t0.07352\n",
      "\tMAE:\t2619.08152\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "\tTraining:\n",
      "\tLoss:\t0.07195\n",
      "\tMAE:\t2541.15920\n",
      "\tValidation:\n",
      "\tLoss:\t0.07184\n",
      "\tMAE:\t2569.39327\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "\tTraining:\n",
      "\tLoss:\t0.06882\n",
      "\tMAE:\t2532.58582\n",
      "\tValidation:\n",
      "\tLoss:\t0.06835\n",
      "\tMAE:\t2514.68293\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "\tTraining:\n",
      "\tLoss:\t0.06857\n",
      "\tMAE:\t2542.51738\n",
      "\tValidation:\n",
      "\tLoss:\t0.06638\n",
      "\tMAE:\t2468.71562\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "\tTraining:\n",
      "\tLoss:\t0.07189\n",
      "\tMAE:\t2532.68306\n",
      "\tValidation:\n",
      "\tLoss:\t0.06736\n",
      "\tMAE:\t2501.94437\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "\tTraining:\n",
      "\tLoss:\t0.06840\n",
      "\tMAE:\t2538.29090\n",
      "\tValidation:\n",
      "\tLoss:\t0.06944\n",
      "\tMAE:\t2539.24677\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "\tTraining:\n",
      "\tLoss:\t0.06798\n",
      "\tMAE:\t2532.96114\n",
      "\tValidation:\n",
      "\tLoss:\t0.06742\n",
      "\tMAE:\t2498.08545\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "\tTraining:\n",
      "\tLoss:\t0.07082\n",
      "\tMAE:\t2515.70950\n",
      "\tValidation:\n",
      "\tLoss:\t0.07011\n",
      "\tMAE:\t2569.04628\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "\tTraining:\n",
      "\tLoss:\t0.07194\n",
      "\tMAE:\t2568.67766\n",
      "\tValidation:\n",
      "\tLoss:\t0.06513\n",
      "\tMAE:\t2445.59100\n",
      "\n",
      "\n",
      "Epoch 20\n",
      "\tTraining:\n",
      "\tLoss:\t0.06940\n",
      "\tMAE:\t2580.31875\n",
      "\tValidation:\n",
      "\tLoss:\t0.06472\n",
      "\tMAE:\t2448.33517\n",
      "\n",
      "\n",
      "Epoch 21\n",
      "\tTraining:\n",
      "\tLoss:\t0.06731\n",
      "\tMAE:\t2509.42190\n",
      "\tValidation:\n",
      "\tLoss:\t0.06748\n",
      "\tMAE:\t2510.55996\n",
      "\n",
      "\n",
      "Epoch 22\n",
      "\tTraining:\n",
      "\tLoss:\t0.06740\n",
      "\tMAE:\t2516.81031\n",
      "\tValidation:\n",
      "\tLoss:\t0.06739\n",
      "\tMAE:\t2503.59925\n",
      "\n",
      "\n",
      "Epoch 23\n",
      "\tTraining:\n",
      "\tLoss:\t0.06838\n",
      "\tMAE:\t2490.49854\n",
      "\tValidation:\n",
      "\tLoss:\t0.06407\n",
      "\tMAE:\t2445.33894\n",
      "\n",
      "\n",
      "Epoch 24\n",
      "\tTraining:\n",
      "\tLoss:\t0.06673\n",
      "\tMAE:\t2496.70154\n",
      "\tValidation:\n",
      "\tLoss:\t0.06630\n",
      "\tMAE:\t2470.01733\n",
      "\n",
      "\n",
      "Epoch 25\n",
      "\tTraining:\n",
      "\tLoss:\t0.06365\n",
      "\tMAE:\t2475.11760\n",
      "\tValidation:\n",
      "\tLoss:\t0.06412\n",
      "\tMAE:\t2426.84357\n",
      "\n",
      "\n",
      "Epoch 26\n",
      "\tTraining:\n",
      "\tLoss:\t0.06915\n",
      "\tMAE:\t2498.73734\n",
      "\tValidation:\n",
      "\tLoss:\t0.07345\n",
      "\tMAE:\t2594.31548\n",
      "\n",
      "\n",
      "Epoch 27\n",
      "\tTraining:\n",
      "\tLoss:\t0.06898\n",
      "\tMAE:\t2537.29683\n",
      "\tValidation:\n",
      "\tLoss:\t0.06917\n",
      "\tMAE:\t2529.00575\n",
      "\n",
      "\n",
      "Epoch 28\n",
      "\tTraining:\n",
      "\tLoss:\t0.06713\n",
      "\tMAE:\t2458.91249\n",
      "\tValidation:\n",
      "\tLoss:\t0.06824\n",
      "\tMAE:\t2510.28279\n",
      "\n",
      "\n",
      "Epoch 29\n",
      "\tTraining:\n",
      "\tLoss:\t0.06922\n",
      "\tMAE:\t2560.78754\n",
      "\tValidation:\n",
      "\tLoss:\t0.06269\n",
      "\tMAE:\t2390.48607\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist_train, hist_val = Train(model,30,opt,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbff663f59a24350bdb017672a9cbfb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tTraining:\n",
      "\tLoss:\t0.05614\n",
      "\tMAE:\t2198.59073\n",
      "\tValidation:\n",
      "\tLoss:\t0.06105\n",
      "\tMAE:\t2373.98270\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "\tTraining:\n",
      "\tLoss:\t0.05314\n",
      "\tMAE:\t2184.75055\n",
      "\tValidation:\n",
      "\tLoss:\t0.06140\n",
      "\tMAE:\t2374.25465\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "\tTraining:\n",
      "\tLoss:\t0.05312\n",
      "\tMAE:\t2147.96346\n",
      "\tValidation:\n",
      "\tLoss:\t0.05988\n",
      "\tMAE:\t2346.50860\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "\tTraining:\n",
      "\tLoss:\t0.05483\n",
      "\tMAE:\t2242.91820\n",
      "\tValidation:\n",
      "\tLoss:\t0.06109\n",
      "\tMAE:\t2378.51683\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "\tTraining:\n",
      "\tLoss:\t0.05473\n",
      "\tMAE:\t2208.90137\n",
      "\tValidation:\n",
      "\tLoss:\t0.06121\n",
      "\tMAE:\t2362.67883\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "\tTraining:\n",
      "\tLoss:\t0.05433\n",
      "\tMAE:\t2162.00854\n",
      "\tValidation:\n",
      "\tLoss:\t0.06283\n",
      "\tMAE:\t2405.14177\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "\tTraining:\n",
      "\tLoss:\t0.05808\n",
      "\tMAE:\t2277.76421\n",
      "\tValidation:\n",
      "\tLoss:\t0.05812\n",
      "\tMAE:\t2308.88526\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "\tTraining:\n",
      "\tLoss:\t0.05702\n",
      "\tMAE:\t2231.62732\n",
      "\tValidation:\n",
      "\tLoss:\t0.05784\n",
      "\tMAE:\t2306.05314\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "\tTraining:\n",
      "\tLoss:\t0.05393\n",
      "\tMAE:\t2197.99693\n",
      "\tValidation:\n",
      "\tLoss:\t0.05859\n",
      "\tMAE:\t2311.75131\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "\tTraining:\n",
      "\tLoss:\t0.05234\n",
      "\tMAE:\t2174.60010\n",
      "\tValidation:\n",
      "\tLoss:\t0.06019\n",
      "\tMAE:\t2338.11874\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "\tTraining:\n",
      "\tLoss:\t0.05367\n",
      "\tMAE:\t2186.17806\n",
      "\tValidation:\n",
      "\tLoss:\t0.06222\n",
      "\tMAE:\t2375.62630\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "\tTraining:\n",
      "\tLoss:\t0.05547\n",
      "\tMAE:\t2224.55802\n",
      "\tValidation:\n",
      "\tLoss:\t0.06145\n",
      "\tMAE:\t2370.57428\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "\tTraining:\n",
      "\tLoss:\t0.05592\n",
      "\tMAE:\t2211.77118\n",
      "\tValidation:\n",
      "\tLoss:\t0.05991\n",
      "\tMAE:\t2344.55089\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "\tTraining:\n",
      "\tLoss:\t0.05399\n",
      "\tMAE:\t2165.19992\n",
      "\tValidation:\n",
      "\tLoss:\t0.05822\n",
      "\tMAE:\t2302.21549\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "\tTraining:\n",
      "\tLoss:\t0.05513\n",
      "\tMAE:\t2216.54230\n",
      "\tValidation:\n",
      "\tLoss:\t0.06034\n",
      "\tMAE:\t2356.25317\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist_train, hist_val = Train(model,15,opt,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval:\n",
      "\tLoss:\t0.06325\n",
      "\tMAE:\t2367.20992\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('~/gensim-data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300\n",
    "embedding_matrix = np.zeros((len(tokens), emb_dim))\n",
    "for i,word  in enumerate(tokens):\n",
    "    try:\n",
    "        embedding_vector = word2vec[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),emb_dim)\n",
    "embedding_matrix = torch.tensor(embedding_matrix,dtype=torch.float32)\n",
    "del word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleEncoder(nn.Module):\n",
    "    def __init__(self, input_size, sizes=[48,48]):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, sizes[0], kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(input_size, sizes[1], kernel_size=5, padding=1)\n",
    "        self.bn = nn.BatchNorm1d(sum(sizes))\n",
    "        self.pool1 = GlobalMaxPooling()        \n",
    "        self.dense = nn.Linear(sum(sizes), sum(sizes))\n",
    "\n",
    "    def forward(self, emb):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, size1+size2]\n",
    "        \"\"\"\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        h = torch.transpose(emb, 1, 2)\n",
    "        \n",
    "        # Apply the layers as defined above. Add some ReLUs before dense.\n",
    "        h1 = self.conv1(h)\n",
    "        h1 = self.pool1(h1)\n",
    "        h2 = self.conv2(h)\n",
    "        h2 = self.pool1(h2)\n",
    "        h = torch.cat([h1,h2], dim=1)\n",
    "        h = self.bn(h)\n",
    "        h = F.relu(self.dense(h)) \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an encoder for job descriptions.\n",
    "# Use any means you want so long as it's torch.nn.Module.\n",
    "class DescriptionEncoder(nn.Module):\n",
    "    def __init__(self, input_size, sizes=[48,48,48]):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, sizes[0], kernel_size=5, padding=1)\n",
    "        self.conv2 = nn.Conv1d(input_size, sizes[1], kernel_size=7, padding=1)\n",
    "        self.conv3 = nn.Conv1d(input_size, sizes[2], kernel_size=9, padding=1)\n",
    "        self.conv1_2 = nn.Conv1d(sizes[0],sizes[0], kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv1d(sizes[1],sizes[1], kernel_size=3, padding=1)\n",
    "        self.pool1 = GlobalMaxPooling()    \n",
    "        self.bn = nn.BatchNorm1d(sum(sizes))\n",
    "        self.dense = nn.Linear(sum(sizes), sum(sizes))\n",
    "        \n",
    "\n",
    "    def forward(self, emb):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        h = torch.transpose(emb, 1, 2)\n",
    "        # Apply the layers as defined above. Add some ReLUs before dense.\n",
    "        h1 = self.conv1(h)\n",
    "        h1 = self.conv1_2(h1)\n",
    "        h1 = self.pool1(h1)\n",
    "        h2 = self.conv2(h)\n",
    "        h2 = self.conv2_2(h2)\n",
    "        h2 = self.pool1(h2)\n",
    "        h3 = self.conv3(h)\n",
    "        h3 = self.pool1(h3)\n",
    "        h = torch.cat([h1,h2,h3], dim=1)\n",
    "        h = self.bn(h)\n",
    "        h = F.relu(self.dense(h))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This class does all the steps from (title, desc, categorical) features -> predicted target\n",
    "    It unites title & desc encoders you defined above as long as some layers for head and categorical branch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_matrix, n_tokens=len(tokens), \n",
    "                 n_cat_features=len(categorical_vectorizer.vocabulary_), \n",
    "                 t_sizes = [48,48],d_sizes = [48,48,48]):\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.emb.padding_idx = PAD_IX\n",
    "        input_size = embedding_matrix.shape[1]\n",
    "        self.title_encoder = TitleEncoder(input_size,sizes=t_sizes)\n",
    "        self.desc_encoder = DescriptionEncoder(input_size,sizes=d_sizes)\n",
    "        \n",
    "        \n",
    "        # define layers for categorical features. A few dense layers would do.\n",
    "        l_dims = [300,100,20]\n",
    "        self.cat_encoder = CategoricalEncoder(n_cat_features,l_dims)\n",
    "        \n",
    "        # define \"output\" layers that process depend the three encoded vectors into answer\n",
    "        n_concat = sum(t_sizes) + sum(d_sizes) + l_dims[-1]\n",
    "        self.out_layers = OutputLayers(n_concat,[100,80,30])\n",
    "        \n",
    "        \n",
    "    def forward(self, title_ix, desc_ix, cat_features):\n",
    "        \"\"\"\n",
    "        :param title_ix: int32 Variable [batch, title_len], job titles encoded by as_matrix\n",
    "        :param desc_ix:  int32 Variable [batch, desc_len] , job descriptions encoded by as_matrix\n",
    "        :param cat_features: float32 Variable [batch, n_cat_features]\n",
    "        :returns: float32 Variable 1d [batch], predicted log1p-salary\n",
    "        \"\"\"\n",
    "        \n",
    "        # process each data source with it's respective encoder\n",
    "        title_emb = self.emb(title_ix)\n",
    "        desc_emb = self.emb(desc_ix)\n",
    "        title_h = self.title_encoder(title_emb)\n",
    "        desc_h = self.desc_encoder(desc_emb)\n",
    "        \n",
    "        # apply categorical encoder\n",
    "        cat_h = self.cat_encoder(cat_features)\n",
    "        \n",
    "        # concatenate all vectors together...\n",
    "        joint_h = torch.cat([title_h, desc_h, cat_h], dim=1)\n",
    "        \n",
    "        # ... and stack a few more layers at the top\n",
    "        output = self.out_layers(joint_h)\n",
    "        \n",
    "        # Note 1: do not forget to select first columns, [:, 0], to get to 1d outputs\n",
    "        # Note 2: please do not use output nonlinearities.\n",
    "        \n",
    "        return output[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullNetwork(\n",
       "  (emb): Embedding(32456, 300, padding_idx=1)\n",
       "  (title_encoder): TitleEncoder(\n",
       "    (conv1): Conv1d(300, 48, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(300, 48, kernel_size=(5,), stride=(1,), padding=(1,))\n",
       "    (bn): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pool1): GlobalMaxPooling()\n",
       "    (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "  )\n",
       "  (desc_encoder): DescriptionEncoder(\n",
       "    (conv1): Conv1d(300, 48, kernel_size=(5,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(300, 48, kernel_size=(7,), stride=(1,), padding=(1,))\n",
       "    (conv3): Conv1d(300, 48, kernel_size=(9,), stride=(1,), padding=(1,))\n",
       "    (conv1_2): Conv1d(48, 48, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2_2): Conv1d(48, 48, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (pool1): GlobalMaxPooling()\n",
       "    (bn): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dense): Linear(in_features=144, out_features=144, bias=True)\n",
       "  )\n",
       "  (cat_encoder): CategoricalEncoder(\n",
       "    (dense): ModuleList(\n",
       "      (0): Linear(in_features=3768, out_features=300, bias=True)\n",
       "      (1): Linear(in_features=300, out_features=100, bias=True)\n",
       "      (2): Linear(in_features=100, out_features=20, bias=True)\n",
       "    )\n",
       "    (bn): ModuleList(\n",
       "      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (out_layers): OutputLayers(\n",
       "    (dense): ModuleList(\n",
       "      (0): Linear(in_features=260, out_features=100, bias=True)\n",
       "      (1): Linear(in_features=100, out_features=80, bias=True)\n",
       "      (2): Linear(in_features=80, out_features=30, bias=True)\n",
       "    )\n",
       "    (bn): ModuleList(\n",
       "      (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (last_layer): Linear(in_features=30, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FullNetwork(embedding_matrix)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=5, gamma=0.9)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Added scheduler and saving best model to train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def Train(model, num_epochs, opt, scheduler, batch_size=32, batches_per_epoch=100):\n",
    "    history_train = []\n",
    "    history_val = []\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    best_MAE = 999999\n",
    "    print(\"\\n\\n\")\n",
    "    for epoch_i in tnrange(num_epochs):\n",
    "        print(\"Epoch \"+str(epoch_i))\n",
    "        print(\"\\tTraining:\")\n",
    "        train_loss = train_mae = train_batches = 0\n",
    "        model.train(True)\n",
    "\n",
    "        for batch in iterate_minibatches(data_train, batch_size=batch_size, max_batches=batches_per_epoch):\n",
    "\n",
    "            title_ix = torch.tensor(batch[\"Title\"],dtype=torch.long,device=device)\n",
    "            desc_ix = torch.tensor(batch[\"FullDescription\"],dtype=torch.long,device=device)\n",
    "            cat_features = torch.tensor(batch[\"Categorical\"],dtype=torch.float32,device=device)\n",
    "            reference = torch.tensor(batch[target_column],dtype=torch.float32,device=device)\n",
    "            prediction = model(title_ix, desc_ix, cat_features)\n",
    "            loss = compute_loss(reference, prediction)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            train_loss += loss.data.cpu().numpy()\n",
    "            train_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "            train_batches += 1\n",
    "\n",
    "        print(\"\\tLoss:\\t%.5f\" % (train_loss / train_batches))\n",
    "        print(\"\\tMAE:\\t%.5f\" % (train_mae / train_batches))\n",
    "\n",
    "        history_train.append(train_loss / train_batches)\n",
    "\n",
    "        print(\"\\tValidation:\")\n",
    "        val_loss = val_mae = val_batches = 0\n",
    "        model.train(False)\n",
    "\n",
    "        for batch in iterate_minibatches(data_val, batch_size=batch_size, max_batches=batches_per_epoch, shuffle=False):\n",
    "            title_ix = torch.tensor(batch[\"Title\"],dtype=torch.long,device=device)\n",
    "            desc_ix = torch.tensor(batch[\"FullDescription\"],dtype=torch.long,device=device)\n",
    "            cat_features = torch.tensor(batch[\"Categorical\"],dtype=torch.float32,device=device)\n",
    "            reference = torch.tensor(batch[target_column],dtype=torch.float32,device=device)\n",
    "            prediction = model(title_ix, desc_ix, cat_features)\n",
    "            loss = compute_loss(reference, prediction)\n",
    "\n",
    "            val_loss += loss.data.cpu().numpy()\n",
    "            val_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "            val_batches += 1\n",
    "            \n",
    "        MAE = (val_mae/val_batches)\n",
    "        print(\"\\tLoss:\\t%.5f\" % (val_loss / val_batches))\n",
    "        print(\"\\tMAE:\\t%.5f\" % MAE)\n",
    "        print('\\n')\n",
    "        scheduler.step()\n",
    "        history_val.append(val_loss / val_batches)\n",
    "        if MAE < best_MAE:\n",
    "            best_MAE = MAE\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    print(\"Best val MAE: \" + str(best_MAE))\n",
    "    model.load_state_dict(best_model)\n",
    "    return history_train, history_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bd2368e629460497219c4b2b878a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tTraining:\n",
      "\tLoss:\t91.07306\n",
      "\tMAE:\t12586.99382\n",
      "\tValidation:\n",
      "\tLoss:\t78.13385\n",
      "\tMAE:\t12527.62015\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "\tTraining:\n",
      "\tLoss:\t62.77535\n",
      "\tMAE:\t12588.45852\n",
      "\tValidation:\n",
      "\tLoss:\t46.76747\n",
      "\tMAE:\t12516.66520\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "\tTraining:\n",
      "\tLoss:\t31.27086\n",
      "\tMAE:\t12504.87089\n",
      "\tValidation:\n",
      "\tLoss:\t18.60889\n",
      "\tMAE:\t12367.32871\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "\tTraining:\n",
      "\tLoss:\t9.23541\n",
      "\tMAE:\t11586.18229\n",
      "\tValidation:\n",
      "\tLoss:\t4.15013\n",
      "\tMAE:\t10818.67230\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "\tTraining:\n",
      "\tLoss:\t1.58171\n",
      "\tMAE:\t8375.41279\n",
      "\tValidation:\n",
      "\tLoss:\t0.73842\n",
      "\tMAE:\t7008.31568\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "\tTraining:\n",
      "\tLoss:\t0.35080\n",
      "\tMAE:\t5142.73984\n",
      "\tValidation:\n",
      "\tLoss:\t0.20127\n",
      "\tMAE:\t4282.59519\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "\tTraining:\n",
      "\tLoss:\t0.20798\n",
      "\tMAE:\t4300.50745\n",
      "\tValidation:\n",
      "\tLoss:\t0.16827\n",
      "\tMAE:\t3959.10184\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "\tTraining:\n",
      "\tLoss:\t0.19003\n",
      "\tMAE:\t4237.96732\n",
      "\tValidation:\n",
      "\tLoss:\t0.15670\n",
      "\tMAE:\t3756.67371\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "\tTraining:\n",
      "\tLoss:\t0.16872\n",
      "\tMAE:\t3948.24672\n",
      "\tValidation:\n",
      "\tLoss:\t0.15323\n",
      "\tMAE:\t3755.50708\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "\tTraining:\n",
      "\tLoss:\t0.16333\n",
      "\tMAE:\t3930.07411\n",
      "\tValidation:\n",
      "\tLoss:\t0.17777\n",
      "\tMAE:\t3968.45706\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "\tTraining:\n",
      "\tLoss:\t0.16723\n",
      "\tMAE:\t3948.83537\n",
      "\tValidation:\n",
      "\tLoss:\t0.17419\n",
      "\tMAE:\t3878.65532\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "\tTraining:\n",
      "\tLoss:\t0.16476\n",
      "\tMAE:\t3956.45213\n",
      "\tValidation:\n",
      "\tLoss:\t0.13004\n",
      "\tMAE:\t3444.41794\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "\tTraining:\n",
      "\tLoss:\t0.15136\n",
      "\tMAE:\t3712.49491\n",
      "\tValidation:\n",
      "\tLoss:\t0.12833\n",
      "\tMAE:\t3458.83360\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "\tTraining:\n",
      "\tLoss:\t0.15778\n",
      "\tMAE:\t3894.11432\n",
      "\tValidation:\n",
      "\tLoss:\t0.13626\n",
      "\tMAE:\t3555.13274\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "\tTraining:\n",
      "\tLoss:\t0.14506\n",
      "\tMAE:\t3639.60630\n",
      "\tValidation:\n",
      "\tLoss:\t0.12489\n",
      "\tMAE:\t3410.81040\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "\tTraining:\n",
      "\tLoss:\t0.13577\n",
      "\tMAE:\t3614.00005\n",
      "\tValidation:\n",
      "\tLoss:\t0.11407\n",
      "\tMAE:\t3260.76042\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "\tTraining:\n",
      "\tLoss:\t0.12753\n",
      "\tMAE:\t3376.36493\n",
      "\tValidation:\n",
      "\tLoss:\t0.11568\n",
      "\tMAE:\t3243.99236\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "\tTraining:\n",
      "\tLoss:\t0.12659\n",
      "\tMAE:\t3396.61874\n",
      "\tValidation:\n",
      "\tLoss:\t0.11150\n",
      "\tMAE:\t3187.85816\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "\tTraining:\n",
      "\tLoss:\t0.13354\n",
      "\tMAE:\t3552.08244\n",
      "\tValidation:\n",
      "\tLoss:\t0.12768\n",
      "\tMAE:\t3381.22461\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "\tTraining:\n",
      "\tLoss:\t0.13588\n",
      "\tMAE:\t3606.73677\n",
      "\tValidation:\n",
      "\tLoss:\t0.11478\n",
      "\tMAE:\t3215.22979\n",
      "\n",
      "\n",
      "Epoch 20\n",
      "\tTraining:\n",
      "\tLoss:\t0.13467\n",
      "\tMAE:\t3497.03419\n",
      "\tValidation:\n",
      "\tLoss:\t0.12707\n",
      "\tMAE:\t3431.68967\n",
      "\n",
      "\n",
      "Epoch 21\n",
      "\tTraining:\n",
      "\tLoss:\t0.12703\n",
      "\tMAE:\t3361.83908\n",
      "\tValidation:\n",
      "\tLoss:\t0.12761\n",
      "\tMAE:\t3417.75073\n",
      "\n",
      "\n",
      "Epoch 22\n",
      "\tTraining:\n",
      "\tLoss:\t0.11820\n",
      "\tMAE:\t3278.32351\n",
      "\tValidation:\n",
      "\tLoss:\t0.11987\n",
      "\tMAE:\t3312.27472\n",
      "\n",
      "\n",
      "Epoch 23\n",
      "\tTraining:\n",
      "\tLoss:\t0.12838\n",
      "\tMAE:\t3477.14216\n",
      "\tValidation:\n",
      "\tLoss:\t0.13107\n",
      "\tMAE:\t3402.04212\n",
      "\n",
      "\n",
      "Epoch 24\n",
      "\tTraining:\n",
      "\tLoss:\t0.11595\n",
      "\tMAE:\t3345.18725\n",
      "\tValidation:\n",
      "\tLoss:\t0.10525\n",
      "\tMAE:\t3103.83526\n",
      "\n",
      "\n",
      "Epoch 25\n",
      "\tTraining:\n",
      "\tLoss:\t0.11465\n",
      "\tMAE:\t3137.55421\n",
      "\tValidation:\n",
      "\tLoss:\t0.12594\n",
      "\tMAE:\t3388.93190\n",
      "\n",
      "\n",
      "Epoch 26\n",
      "\tTraining:\n",
      "\tLoss:\t0.11356\n",
      "\tMAE:\t3299.16455\n",
      "\tValidation:\n",
      "\tLoss:\t0.11271\n",
      "\tMAE:\t3206.09696\n",
      "\n",
      "\n",
      "Epoch 27\n",
      "\tTraining:\n",
      "\tLoss:\t0.12680\n",
      "\tMAE:\t3485.35185\n",
      "\tValidation:\n",
      "\tLoss:\t0.11406\n",
      "\tMAE:\t3269.85010\n",
      "\n",
      "\n",
      "Epoch 28\n",
      "\tTraining:\n",
      "\tLoss:\t0.11188\n",
      "\tMAE:\t3269.20836\n",
      "\tValidation:\n",
      "\tLoss:\t0.09729\n",
      "\tMAE:\t2999.95567\n",
      "\n",
      "\n",
      "Epoch 29\n",
      "\tTraining:\n",
      "\tLoss:\t0.11507\n",
      "\tMAE:\t3197.40959\n",
      "\tValidation:\n",
      "\tLoss:\t0.10553\n",
      "\tMAE:\t3132.92246\n",
      "\n",
      "\n",
      "Epoch 30\n",
      "\tTraining:\n",
      "\tLoss:\t0.11127\n",
      "\tMAE:\t3194.56124\n",
      "\tValidation:\n",
      "\tLoss:\t0.10981\n",
      "\tMAE:\t3197.27688\n",
      "\n",
      "\n",
      "Epoch 31\n",
      "\tTraining:\n",
      "\tLoss:\t0.11801\n",
      "\tMAE:\t3277.79415\n",
      "\tValidation:\n",
      "\tLoss:\t0.10153\n",
      "\tMAE:\t3064.58553\n",
      "\n",
      "\n",
      "Epoch 32\n",
      "\tTraining:\n",
      "\tLoss:\t0.11314\n",
      "\tMAE:\t3165.17011\n",
      "\tValidation:\n",
      "\tLoss:\t0.10395\n",
      "\tMAE:\t3086.22766\n",
      "\n",
      "\n",
      "Epoch 33\n",
      "\tTraining:\n",
      "\tLoss:\t0.11518\n",
      "\tMAE:\t3311.14260\n",
      "\tValidation:\n",
      "\tLoss:\t0.10322\n",
      "\tMAE:\t3014.86115\n",
      "\n",
      "\n",
      "Epoch 34\n",
      "\tTraining:\n",
      "\tLoss:\t0.10256\n",
      "\tMAE:\t3121.20923\n",
      "\tValidation:\n",
      "\tLoss:\t0.13140\n",
      "\tMAE:\t3452.27755\n",
      "\n",
      "\n",
      "Epoch 35\n",
      "\tTraining:\n",
      "\tLoss:\t0.10776\n",
      "\tMAE:\t3155.34273\n",
      "\tValidation:\n",
      "\tLoss:\t0.10589\n",
      "\tMAE:\t3130.45400\n",
      "\n",
      "\n",
      "Epoch 36\n",
      "\tTraining:\n",
      "\tLoss:\t0.11302\n",
      "\tMAE:\t3224.56260\n",
      "\tValidation:\n",
      "\tLoss:\t0.09894\n",
      "\tMAE:\t3045.01513\n",
      "\n",
      "\n",
      "Epoch 37\n",
      "\tTraining:\n",
      "\tLoss:\t0.10957\n",
      "\tMAE:\t3268.51768\n",
      "\tValidation:\n",
      "\tLoss:\t0.10753\n",
      "\tMAE:\t3154.53072\n",
      "\n",
      "\n",
      "Epoch 38\n",
      "\tTraining:\n",
      "\tLoss:\t0.11016\n",
      "\tMAE:\t3174.20180\n",
      "\tValidation:\n",
      "\tLoss:\t0.11913\n",
      "\tMAE:\t3271.19317\n",
      "\n",
      "\n",
      "Epoch 39\n",
      "\tTraining:\n",
      "\tLoss:\t0.10810\n",
      "\tMAE:\t3148.41528\n",
      "\tValidation:\n",
      "\tLoss:\t0.10480\n",
      "\tMAE:\t3098.64326\n",
      "\n",
      "\n",
      "Epoch 40\n",
      "\tTraining:\n",
      "\tLoss:\t0.10549\n",
      "\tMAE:\t3168.97816\n",
      "\tValidation:\n",
      "\tLoss:\t0.11609\n",
      "\tMAE:\t3263.77802\n",
      "\n",
      "\n",
      "Epoch 41\n",
      "\tTraining:\n",
      "\tLoss:\t0.10354\n",
      "\tMAE:\t3119.28711\n",
      "\tValidation:\n",
      "\tLoss:\t0.09631\n",
      "\tMAE:\t2969.98552\n",
      "\n",
      "\n",
      "Epoch 42\n",
      "\tTraining:\n",
      "\tLoss:\t0.10323\n",
      "\tMAE:\t3056.56193\n",
      "\tValidation:\n",
      "\tLoss:\t0.11690\n",
      "\tMAE:\t3266.11238\n",
      "\n",
      "\n",
      "Epoch 43\n",
      "\tTraining:\n",
      "\tLoss:\t0.09969\n",
      "\tMAE:\t3122.49304\n",
      "\tValidation:\n",
      "\tLoss:\t0.09618\n",
      "\tMAE:\t2956.42405\n",
      "\n",
      "\n",
      "Epoch 44\n",
      "\tTraining:\n",
      "\tLoss:\t0.10360\n",
      "\tMAE:\t3130.34401\n",
      "\tValidation:\n",
      "\tLoss:\t0.09684\n",
      "\tMAE:\t2993.48763\n",
      "\n",
      "\n",
      "Epoch 45\n",
      "\tTraining:\n",
      "\tLoss:\t0.10033\n",
      "\tMAE:\t3083.63582\n",
      "\tValidation:\n",
      "\tLoss:\t0.09470\n",
      "\tMAE:\t2948.38382\n",
      "\n",
      "\n",
      "Epoch 46\n",
      "\tTraining:\n",
      "\tLoss:\t0.10618\n",
      "\tMAE:\t3111.10337\n",
      "\tValidation:\n",
      "\tLoss:\t0.10095\n",
      "\tMAE:\t3035.74394\n",
      "\n",
      "\n",
      "Epoch 47\n",
      "\tTraining:\n",
      "\tLoss:\t0.10372\n",
      "\tMAE:\t3089.61141\n",
      "\tValidation:\n",
      "\tLoss:\t0.10195\n",
      "\tMAE:\t3068.27346\n",
      "\n",
      "\n",
      "Epoch 48\n",
      "\tTraining:\n",
      "\tLoss:\t0.09845\n",
      "\tMAE:\t3092.22970\n",
      "\tValidation:\n",
      "\tLoss:\t0.09904\n",
      "\tMAE:\t2990.63510\n",
      "\n",
      "\n",
      "Epoch 49\n",
      "\tTraining:\n",
      "\tLoss:\t0.09553\n",
      "\tMAE:\t3051.67018\n",
      "\tValidation:\n",
      "\tLoss:\t0.09072\n",
      "\tMAE:\t2860.40938\n",
      "\n",
      "\n",
      "Epoch 50\n",
      "\tTraining:\n",
      "\tLoss:\t0.09488\n",
      "\tMAE:\t3015.03594\n",
      "\tValidation:\n",
      "\tLoss:\t0.08653\n",
      "\tMAE:\t2793.76232\n",
      "\n",
      "\n",
      "Epoch 51\n",
      "\tTraining:\n",
      "\tLoss:\t0.09676\n",
      "\tMAE:\t2943.97896\n",
      "\tValidation:\n",
      "\tLoss:\t0.11000\n",
      "\tMAE:\t3161.04200\n",
      "\n",
      "\n",
      "Epoch 52\n",
      "\tTraining:\n",
      "\tLoss:\t0.10251\n",
      "\tMAE:\t3109.62287\n",
      "\tValidation:\n",
      "\tLoss:\t0.08992\n",
      "\tMAE:\t2872.43699\n",
      "\n",
      "\n",
      "Epoch 53\n",
      "\tTraining:\n",
      "\tLoss:\t0.09467\n",
      "\tMAE:\t2939.19467\n",
      "\tValidation:\n",
      "\tLoss:\t0.09683\n",
      "\tMAE:\t2983.61184\n",
      "\n",
      "\n",
      "Epoch 54\n",
      "\tTraining:\n",
      "\tLoss:\t0.09498\n",
      "\tMAE:\t2922.97145\n",
      "\tValidation:\n",
      "\tLoss:\t0.09100\n",
      "\tMAE:\t2904.76259\n",
      "\n",
      "\n",
      "Epoch 55\n",
      "\tTraining:\n",
      "\tLoss:\t0.09574\n",
      "\tMAE:\t2947.09249\n",
      "\tValidation:\n",
      "\tLoss:\t0.08798\n",
      "\tMAE:\t2843.31593\n",
      "\n",
      "\n",
      "Epoch 56\n",
      "\tTraining:\n",
      "\tLoss:\t0.09142\n",
      "\tMAE:\t2883.39517\n",
      "\tValidation:\n",
      "\tLoss:\t0.09392\n",
      "\tMAE:\t2916.76810\n",
      "\n",
      "\n",
      "Epoch 57\n",
      "\tTraining:\n",
      "\tLoss:\t0.09478\n",
      "\tMAE:\t2927.21469\n",
      "\tValidation:\n",
      "\tLoss:\t0.09961\n",
      "\tMAE:\t3021.41661\n",
      "\n",
      "\n",
      "Epoch 58\n",
      "\tTraining:\n",
      "\tLoss:\t0.09297\n",
      "\tMAE:\t2962.33382\n",
      "\tValidation:\n",
      "\tLoss:\t0.09050\n",
      "\tMAE:\t2861.16158\n",
      "\n",
      "\n",
      "Epoch 59\n",
      "\tTraining:\n",
      "\tLoss:\t0.09723\n",
      "\tMAE:\t3003.18035\n",
      "\tValidation:\n",
      "\tLoss:\t0.09547\n",
      "\tMAE:\t2961.19159\n",
      "\n",
      "\n",
      "Epoch 60\n",
      "\tTraining:\n",
      "\tLoss:\t0.09164\n",
      "\tMAE:\t2954.59834\n",
      "\tValidation:\n",
      "\tLoss:\t0.08692\n",
      "\tMAE:\t2774.21781\n",
      "\n",
      "\n",
      "Epoch 61\n",
      "\tTraining:\n",
      "\tLoss:\t0.09468\n",
      "\tMAE:\t2909.19175\n",
      "\tValidation:\n",
      "\tLoss:\t0.10316\n",
      "\tMAE:\t3134.95478\n",
      "\n",
      "\n",
      "Epoch 62\n",
      "\tTraining:\n",
      "\tLoss:\t0.09426\n",
      "\tMAE:\t2938.09901\n",
      "\tValidation:\n",
      "\tLoss:\t0.09796\n",
      "\tMAE:\t2994.46634\n",
      "\n",
      "\n",
      "Epoch 63\n",
      "\tTraining:\n",
      "\tLoss:\t0.09326\n",
      "\tMAE:\t2957.34361\n",
      "\tValidation:\n",
      "\tLoss:\t0.09259\n",
      "\tMAE:\t2924.87443\n",
      "\n",
      "\n",
      "Epoch 64\n",
      "\tTraining:\n",
      "\tLoss:\t0.09383\n",
      "\tMAE:\t2948.85873\n",
      "\tValidation:\n",
      "\tLoss:\t0.08648\n",
      "\tMAE:\t2801.96339\n",
      "\n",
      "\n",
      "Epoch 65\n",
      "\tTraining:\n",
      "\tLoss:\t0.09295\n",
      "\tMAE:\t2889.29335\n",
      "\tValidation:\n",
      "\tLoss:\t0.09225\n",
      "\tMAE:\t2905.63494\n",
      "\n",
      "\n",
      "Epoch 66\n",
      "\tTraining:\n",
      "\tLoss:\t0.09472\n",
      "\tMAE:\t2928.63616\n",
      "\tValidation:\n",
      "\tLoss:\t0.08559\n",
      "\tMAE:\t2793.62793\n",
      "\n",
      "\n",
      "Epoch 67\n",
      "\tTraining:\n",
      "\tLoss:\t0.09224\n",
      "\tMAE:\t2947.38537\n",
      "\tValidation:\n",
      "\tLoss:\t0.08020\n",
      "\tMAE:\t2707.81978\n",
      "\n",
      "\n",
      "Epoch 68\n",
      "\tTraining:\n",
      "\tLoss:\t0.08674\n",
      "\tMAE:\t2833.05700\n",
      "\tValidation:\n",
      "\tLoss:\t0.08647\n",
      "\tMAE:\t2818.47283\n",
      "\n",
      "\n",
      "Epoch 69\n",
      "\tTraining:\n",
      "\tLoss:\t0.08740\n",
      "\tMAE:\t2888.70400\n",
      "\tValidation:\n",
      "\tLoss:\t0.09034\n",
      "\tMAE:\t2884.63609\n",
      "\n",
      "\n",
      "Epoch 70\n",
      "\tTraining:\n",
      "\tLoss:\t0.09139\n",
      "\tMAE:\t2880.99841\n",
      "\tValidation:\n",
      "\tLoss:\t0.09314\n",
      "\tMAE:\t2914.10096\n",
      "\n",
      "\n",
      "Epoch 71\n",
      "\tTraining:\n",
      "\tLoss:\t0.08947\n",
      "\tMAE:\t2893.63240\n",
      "\tValidation:\n",
      "\tLoss:\t0.10723\n",
      "\tMAE:\t3184.90113\n",
      "\n",
      "\n",
      "Epoch 72\n",
      "\tTraining:\n",
      "\tLoss:\t0.08746\n",
      "\tMAE:\t2815.20926\n",
      "\tValidation:\n",
      "\tLoss:\t0.08643\n",
      "\tMAE:\t2818.75042\n",
      "\n",
      "\n",
      "Epoch 73\n",
      "\tTraining:\n",
      "\tLoss:\t0.08552\n",
      "\tMAE:\t2825.76606\n",
      "\tValidation:\n",
      "\tLoss:\t0.08678\n",
      "\tMAE:\t2850.68768\n",
      "\n",
      "\n",
      "Epoch 74\n",
      "\tTraining:\n",
      "\tLoss:\t0.08724\n",
      "\tMAE:\t2868.52505\n",
      "\tValidation:\n",
      "\tLoss:\t0.08279\n",
      "\tMAE:\t2762.75796\n",
      "\n",
      "\n",
      "Epoch 75\n",
      "\tTraining:\n",
      "\tLoss:\t0.09045\n",
      "\tMAE:\t2837.56191\n",
      "\tValidation:\n",
      "\tLoss:\t0.08723\n",
      "\tMAE:\t2835.74218\n",
      "\n",
      "\n",
      "Epoch 76\n",
      "\tTraining:\n",
      "\tLoss:\t0.08857\n",
      "\tMAE:\t2865.07118\n",
      "\tValidation:\n",
      "\tLoss:\t0.09391\n",
      "\tMAE:\t2915.19353\n",
      "\n",
      "\n",
      "Epoch 77\n",
      "\tTraining:\n",
      "\tLoss:\t0.08772\n",
      "\tMAE:\t2848.80364\n",
      "\tValidation:\n",
      "\tLoss:\t0.07978\n",
      "\tMAE:\t2706.36224\n",
      "\n",
      "\n",
      "Epoch 78\n",
      "\tTraining:\n",
      "\tLoss:\t0.08480\n",
      "\tMAE:\t2787.51459\n",
      "\tValidation:\n",
      "\tLoss:\t0.08946\n",
      "\tMAE:\t2866.31632\n",
      "\n",
      "\n",
      "Epoch 79\n",
      "\tTraining:\n",
      "\tLoss:\t0.08653\n",
      "\tMAE:\t2847.62204\n",
      "\tValidation:\n",
      "\tLoss:\t0.08189\n",
      "\tMAE:\t2739.26789\n",
      "\n",
      "\n",
      "Epoch 80\n",
      "\tTraining:\n",
      "\tLoss:\t0.08150\n",
      "\tMAE:\t2688.45790\n",
      "\tValidation:\n",
      "\tLoss:\t0.09082\n",
      "\tMAE:\t2866.08926\n",
      "\n",
      "\n",
      "Epoch 81\n",
      "\tTraining:\n",
      "\tLoss:\t0.08647\n",
      "\tMAE:\t2817.57762\n",
      "\tValidation:\n",
      "\tLoss:\t0.09045\n",
      "\tMAE:\t2895.47610\n",
      "\n",
      "\n",
      "Epoch 82\n",
      "\tTraining:\n",
      "\tLoss:\t0.07990\n",
      "\tMAE:\t2652.66701\n",
      "\tValidation:\n",
      "\tLoss:\t0.08353\n",
      "\tMAE:\t2771.63532\n",
      "\n",
      "\n",
      "Epoch 83\n",
      "\tTraining:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:\t0.07912\n",
      "\tMAE:\t2697.78306\n",
      "\tValidation:\n",
      "\tLoss:\t0.08664\n",
      "\tMAE:\t2813.08956\n",
      "\n",
      "\n",
      "Epoch 84\n",
      "\tTraining:\n",
      "\tLoss:\t0.08536\n",
      "\tMAE:\t2818.52240\n",
      "\tValidation:\n",
      "\tLoss:\t0.08565\n",
      "\tMAE:\t2775.76605\n",
      "\n",
      "\n",
      "Epoch 85\n",
      "\tTraining:\n",
      "\tLoss:\t0.07894\n",
      "\tMAE:\t2680.39906\n",
      "\tValidation:\n",
      "\tLoss:\t0.08268\n",
      "\tMAE:\t2748.55891\n",
      "\n",
      "\n",
      "Epoch 86\n",
      "\tTraining:\n",
      "\tLoss:\t0.07693\n",
      "\tMAE:\t2633.39257\n",
      "\tValidation:\n",
      "\tLoss:\t0.08676\n",
      "\tMAE:\t2844.59278\n",
      "\n",
      "\n",
      "Epoch 87\n",
      "\tTraining:\n",
      "\tLoss:\t0.08407\n",
      "\tMAE:\t2773.15558\n",
      "\tValidation:\n",
      "\tLoss:\t0.07752\n",
      "\tMAE:\t2650.96744\n",
      "\n",
      "\n",
      "Epoch 88\n",
      "\tTraining:\n",
      "\tLoss:\t0.08035\n",
      "\tMAE:\t2784.98027\n",
      "\tValidation:\n",
      "\tLoss:\t0.08505\n",
      "\tMAE:\t2828.12587\n",
      "\n",
      "\n",
      "Epoch 89\n",
      "\tTraining:\n",
      "\tLoss:\t0.07976\n",
      "\tMAE:\t2669.53645\n",
      "\tValidation:\n",
      "\tLoss:\t0.08065\n",
      "\tMAE:\t2723.14149\n",
      "\n",
      "\n",
      "Epoch 90\n",
      "\tTraining:\n",
      "\tLoss:\t0.08245\n",
      "\tMAE:\t2718.67437\n",
      "\tValidation:\n",
      "\tLoss:\t0.07750\n",
      "\tMAE:\t2677.58433\n",
      "\n",
      "\n",
      "Epoch 91\n",
      "\tTraining:\n",
      "\tLoss:\t0.08057\n",
      "\tMAE:\t2676.97853\n",
      "\tValidation:\n",
      "\tLoss:\t0.08890\n",
      "\tMAE:\t2878.66916\n",
      "\n",
      "\n",
      "Epoch 92\n",
      "\tTraining:\n",
      "\tLoss:\t0.07605\n",
      "\tMAE:\t2551.58745\n",
      "\tValidation:\n",
      "\tLoss:\t0.08674\n",
      "\tMAE:\t2840.08537\n",
      "\n",
      "\n",
      "Epoch 93\n",
      "\tTraining:\n",
      "\tLoss:\t0.07881\n",
      "\tMAE:\t2681.47219\n",
      "\tValidation:\n",
      "\tLoss:\t0.07946\n",
      "\tMAE:\t2715.36587\n",
      "\n",
      "\n",
      "Epoch 94\n",
      "\tTraining:\n",
      "\tLoss:\t0.07792\n",
      "\tMAE:\t2672.44458\n",
      "\tValidation:\n",
      "\tLoss:\t0.08751\n",
      "\tMAE:\t2876.88267\n",
      "\n",
      "\n",
      "Epoch 95\n",
      "\tTraining:\n",
      "\tLoss:\t0.07311\n",
      "\tMAE:\t2624.17078\n",
      "\tValidation:\n",
      "\tLoss:\t0.08008\n",
      "\tMAE:\t2715.49836\n",
      "\n",
      "\n",
      "Epoch 96\n",
      "\tTraining:\n",
      "\tLoss:\t0.07507\n",
      "\tMAE:\t2584.09138\n",
      "\tValidation:\n",
      "\tLoss:\t0.08726\n",
      "\tMAE:\t2878.79565\n",
      "\n",
      "\n",
      "Epoch 97\n",
      "\tTraining:\n",
      "\tLoss:\t0.08237\n",
      "\tMAE:\t2732.52524\n",
      "\tValidation:\n",
      "\tLoss:\t0.08037\n",
      "\tMAE:\t2729.24452\n",
      "\n",
      "\n",
      "Epoch 98\n",
      "\tTraining:\n",
      "\tLoss:\t0.07588\n",
      "\tMAE:\t2692.34176\n",
      "\tValidation:\n",
      "\tLoss:\t0.07753\n",
      "\tMAE:\t2683.99485\n",
      "\n",
      "\n",
      "Epoch 99\n",
      "\tTraining:\n",
      "\tLoss:\t0.07747\n",
      "\tMAE:\t2722.97608\n",
      "\tValidation:\n",
      "\tLoss:\t0.08338\n",
      "\tMAE:\t2828.08495\n",
      "\n",
      "\n",
      "\n",
      "Best val MAE: 2650.967440185547\n"
     ]
    }
   ],
   "source": [
    "hist_train, hist_val = Train(model,100,opt,exp_lr_scheduler,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67c2551880f40f29ace0aaf488406bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tTraining:\n",
      "\tLoss:\t0.07820\n",
      "\tMAE:\t2647.14776\n",
      "\tValidation:\n",
      "\tLoss:\t0.08780\n",
      "\tMAE:\t2828.13518\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "\tTraining:\n",
      "\tLoss:\t0.07886\n",
      "\tMAE:\t2743.94749\n",
      "\tValidation:\n",
      "\tLoss:\t0.08209\n",
      "\tMAE:\t2749.03337\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "\tTraining:\n",
      "\tLoss:\t0.08254\n",
      "\tMAE:\t2701.54578\n",
      "\tValidation:\n",
      "\tLoss:\t0.08039\n",
      "\tMAE:\t2725.18796\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "\tTraining:\n",
      "\tLoss:\t0.08014\n",
      "\tMAE:\t2751.61451\n",
      "\tValidation:\n",
      "\tLoss:\t0.08005\n",
      "\tMAE:\t2688.67709\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "\tTraining:\n",
      "\tLoss:\t0.07720\n",
      "\tMAE:\t2646.13830\n",
      "\tValidation:\n",
      "\tLoss:\t0.07925\n",
      "\tMAE:\t2683.97893\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "\tTraining:\n",
      "\tLoss:\t0.07814\n",
      "\tMAE:\t2697.73351\n",
      "\tValidation:\n",
      "\tLoss:\t0.08117\n",
      "\tMAE:\t2734.16883\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "\tTraining:\n",
      "\tLoss:\t0.07570\n",
      "\tMAE:\t2678.76307\n",
      "\tValidation:\n",
      "\tLoss:\t0.08692\n",
      "\tMAE:\t2808.59647\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "\tTraining:\n",
      "\tLoss:\t0.07789\n",
      "\tMAE:\t2687.29830\n",
      "\tValidation:\n",
      "\tLoss:\t0.07364\n",
      "\tMAE:\t2591.94751\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "\tTraining:\n",
      "\tLoss:\t0.07454\n",
      "\tMAE:\t2658.63058\n",
      "\tValidation:\n",
      "\tLoss:\t0.08157\n",
      "\tMAE:\t2738.81478\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "\tTraining:\n",
      "\tLoss:\t0.07835\n",
      "\tMAE:\t2582.35870\n",
      "\tValidation:\n",
      "\tLoss:\t0.08465\n",
      "\tMAE:\t2818.17277\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "\tTraining:\n",
      "\tLoss:\t0.07026\n",
      "\tMAE:\t2622.74058\n",
      "\tValidation:\n",
      "\tLoss:\t0.07891\n",
      "\tMAE:\t2695.64503\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "\tTraining:\n",
      "\tLoss:\t0.07505\n",
      "\tMAE:\t2679.74901\n",
      "\tValidation:\n",
      "\tLoss:\t0.08625\n",
      "\tMAE:\t2849.54808\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "\tTraining:\n",
      "\tLoss:\t0.07716\n",
      "\tMAE:\t2639.05760\n",
      "\tValidation:\n",
      "\tLoss:\t0.08899\n",
      "\tMAE:\t2863.66688\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "\tTraining:\n",
      "\tLoss:\t0.07991\n",
      "\tMAE:\t2703.91360\n",
      "\tValidation:\n",
      "\tLoss:\t0.09071\n",
      "\tMAE:\t2903.52631\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "\tTraining:\n",
      "\tLoss:\t0.07314\n",
      "\tMAE:\t2623.84775\n",
      "\tValidation:\n",
      "\tLoss:\t0.07253\n",
      "\tMAE:\t2573.46864\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "\tTraining:\n",
      "\tLoss:\t0.07789\n",
      "\tMAE:\t2711.28454\n",
      "\tValidation:\n",
      "\tLoss:\t0.08848\n",
      "\tMAE:\t2860.02990\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "\tTraining:\n",
      "\tLoss:\t0.07888\n",
      "\tMAE:\t2686.66654\n",
      "\tValidation:\n",
      "\tLoss:\t0.08503\n",
      "\tMAE:\t2801.44010\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "\tTraining:\n",
      "\tLoss:\t0.07947\n",
      "\tMAE:\t2693.09600\n",
      "\tValidation:\n",
      "\tLoss:\t0.08229\n",
      "\tMAE:\t2748.79329\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "\tTraining:\n",
      "\tLoss:\t0.07209\n",
      "\tMAE:\t2579.00972\n",
      "\tValidation:\n",
      "\tLoss:\t0.07722\n",
      "\tMAE:\t2654.76283\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "\tTraining:\n",
      "\tLoss:\t0.08007\n",
      "\tMAE:\t2706.14521\n",
      "\tValidation:\n",
      "\tLoss:\t0.08003\n",
      "\tMAE:\t2721.25764\n",
      "\n",
      "\n",
      "Epoch 20\n",
      "\tTraining:\n",
      "\tLoss:\t0.07138\n",
      "\tMAE:\t2559.66122\n",
      "\tValidation:\n",
      "\tLoss:\t0.08080\n",
      "\tMAE:\t2740.15313\n",
      "\n",
      "\n",
      "Epoch 21\n",
      "\tTraining:\n",
      "\tLoss:\t0.07571\n",
      "\tMAE:\t2569.11988\n",
      "\tValidation:\n",
      "\tLoss:\t0.08322\n",
      "\tMAE:\t2796.81310\n",
      "\n",
      "\n",
      "Epoch 22\n",
      "\tTraining:\n",
      "\tLoss:\t0.08074\n",
      "\tMAE:\t2681.10653\n",
      "\tValidation:\n",
      "\tLoss:\t0.07361\n",
      "\tMAE:\t2604.12534\n",
      "\n",
      "\n",
      "Epoch 23\n",
      "\tTraining:\n",
      "\tLoss:\t0.07062\n",
      "\tMAE:\t2499.89593\n",
      "\tValidation:\n",
      "\tLoss:\t0.07911\n",
      "\tMAE:\t2709.78250\n",
      "\n",
      "\n",
      "Epoch 24\n",
      "\tTraining:\n",
      "\tLoss:\t0.07382\n",
      "\tMAE:\t2588.88725\n",
      "\tValidation:\n",
      "\tLoss:\t0.07865\n",
      "\tMAE:\t2684.45141\n",
      "\n",
      "\n",
      "\n",
      "Best val MAE: 2573.4686389160156\n"
     ]
    }
   ],
   "source": [
    "hist_train, hist_val = Train(model,25,opt,exp_lr_scheduler,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"models/conv_1d_with_emb.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26737d646ba94ab29a869b4c399382a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tTraining:\n",
      "\tLoss:\t0.07256\n",
      "\tMAE:\t2566.90150\n",
      "\tValidation:\n",
      "\tLoss:\t0.08245\n",
      "\tMAE:\t2777.06192\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "\tTraining:\n",
      "\tLoss:\t0.07678\n",
      "\tMAE:\t2619.91044\n",
      "\tValidation:\n",
      "\tLoss:\t0.08177\n",
      "\tMAE:\t2763.23739\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "\tTraining:\n",
      "\tLoss:\t0.07575\n",
      "\tMAE:\t2587.83398\n",
      "\tValidation:\n",
      "\tLoss:\t0.07465\n",
      "\tMAE:\t2626.16877\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "\tTraining:\n",
      "\tLoss:\t0.07408\n",
      "\tMAE:\t2631.72036\n",
      "\tValidation:\n",
      "\tLoss:\t0.07478\n",
      "\tMAE:\t2625.47045\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "\tTraining:\n",
      "\tLoss:\t0.07383\n",
      "\tMAE:\t2609.48277\n",
      "\tValidation:\n",
      "\tLoss:\t0.07954\n",
      "\tMAE:\t2689.73629\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "\tTraining:\n",
      "\tLoss:\t0.07339\n",
      "\tMAE:\t2540.84815\n",
      "\tValidation:\n",
      "\tLoss:\t0.08020\n",
      "\tMAE:\t2741.08755\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "\tTraining:\n",
      "\tLoss:\t0.07491\n",
      "\tMAE:\t2643.25460\n",
      "\tValidation:\n",
      "\tLoss:\t0.07862\n",
      "\tMAE:\t2684.72999\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "\tTraining:\n",
      "\tLoss:\t0.06961\n",
      "\tMAE:\t2466.43403\n",
      "\tValidation:\n",
      "\tLoss:\t0.08026\n",
      "\tMAE:\t2738.66478\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "\tTraining:\n",
      "\tLoss:\t0.07255\n",
      "\tMAE:\t2582.29753\n",
      "\tValidation:\n",
      "\tLoss:\t0.07768\n",
      "\tMAE:\t2674.48300\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "\tTraining:\n",
      "\tLoss:\t0.06856\n",
      "\tMAE:\t2537.52044\n",
      "\tValidation:\n",
      "\tLoss:\t0.08223\n",
      "\tMAE:\t2743.48671\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "\tTraining:\n",
      "\tLoss:\t0.08047\n",
      "\tMAE:\t2672.22631\n",
      "\tValidation:\n",
      "\tLoss:\t0.07704\n",
      "\tMAE:\t2667.48893\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "\tTraining:\n",
      "\tLoss:\t0.07229\n",
      "\tMAE:\t2580.86517\n",
      "\tValidation:\n",
      "\tLoss:\t0.07760\n",
      "\tMAE:\t2679.69794\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "\tTraining:\n",
      "\tLoss:\t0.07237\n",
      "\tMAE:\t2552.14245\n",
      "\tValidation:\n",
      "\tLoss:\t0.07944\n",
      "\tMAE:\t2723.12711\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "\tTraining:\n",
      "\tLoss:\t0.07218\n",
      "\tMAE:\t2575.97132\n",
      "\tValidation:\n",
      "\tLoss:\t0.07884\n",
      "\tMAE:\t2678.29004\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "\tTraining:\n",
      "\tLoss:\t0.07095\n",
      "\tMAE:\t2536.02773\n",
      "\tValidation:\n",
      "\tLoss:\t0.07476\n",
      "\tMAE:\t2629.78864\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "\tTraining:\n",
      "\tLoss:\t0.07082\n",
      "\tMAE:\t2562.08218\n",
      "\tValidation:\n",
      "\tLoss:\t0.07567\n",
      "\tMAE:\t2624.35026\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "\tTraining:\n",
      "\tLoss:\t0.07199\n",
      "\tMAE:\t2565.31625\n",
      "\tValidation:\n",
      "\tLoss:\t0.07356\n",
      "\tMAE:\t2587.39802\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "\tTraining:\n",
      "\tLoss:\t0.07266\n",
      "\tMAE:\t2583.62824\n",
      "\tValidation:\n",
      "\tLoss:\t0.07246\n",
      "\tMAE:\t2571.83721\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "\tTraining:\n",
      "\tLoss:\t0.07105\n",
      "\tMAE:\t2515.77941\n",
      "\tValidation:\n",
      "\tLoss:\t0.07434\n",
      "\tMAE:\t2634.92888\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "\tTraining:\n",
      "\tLoss:\t0.07079\n",
      "\tMAE:\t2589.86305\n",
      "\tValidation:\n",
      "\tLoss:\t0.07381\n",
      "\tMAE:\t2607.62046\n",
      "\n",
      "\n",
      "Epoch 20\n",
      "\tTraining:\n",
      "\tLoss:\t0.07091\n",
      "\tMAE:\t2602.25530\n",
      "\tValidation:\n",
      "\tLoss:\t0.07447\n",
      "\tMAE:\t2602.87839\n",
      "\n",
      "\n",
      "Epoch 21\n",
      "\tTraining:\n",
      "\tLoss:\t0.07359\n",
      "\tMAE:\t2583.08014\n",
      "\tValidation:\n",
      "\tLoss:\t0.07776\n",
      "\tMAE:\t2675.19357\n",
      "\n",
      "\n",
      "Epoch 22\n",
      "\tTraining:\n",
      "\tLoss:\t0.07633\n",
      "\tMAE:\t2549.62522\n",
      "\tValidation:\n",
      "\tLoss:\t0.07827\n",
      "\tMAE:\t2665.53365\n",
      "\n",
      "\n",
      "Epoch 23\n",
      "\tTraining:\n",
      "\tLoss:\t0.07262\n",
      "\tMAE:\t2577.13871\n",
      "\tValidation:\n",
      "\tLoss:\t0.07128\n",
      "\tMAE:\t2533.34812\n",
      "\n",
      "\n",
      "Epoch 24\n",
      "\tTraining:\n",
      "\tLoss:\t0.07050\n",
      "\tMAE:\t2594.02730\n",
      "\tValidation:\n",
      "\tLoss:\t0.07593\n",
      "\tMAE:\t2634.46497\n",
      "\n",
      "\n",
      "\n",
      "Best val MAE: 2533.3481188964843\n"
     ]
    }
   ],
   "source": [
    "hist_train, hist_val = Train(model,25,opt,exp_lr_scheduler,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"models/conv_1d_with_emb.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"models/conv_1d_with_emb.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval:\n",
      "\tLoss:\t0.07851\n",
      "\tMAE:\t2676.77092\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I added couple of layers and word2vec pretrained embedding. But it didn't work so well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
